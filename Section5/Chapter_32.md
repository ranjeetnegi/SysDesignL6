# Chapter 32. Messaging platform

---

# Introduction

Messaging platforms are among the most demanding distributed systems to design and operate. I've spent years building and scaling messaging infrastructure at Google, and I can tell you: the fundamental challenge isn't sending a message from A to Bâ€”any undergraduate can build that. The challenge is doing it reliably for billions of users, with sub-second latency, perfect ordering, exactly-once delivery semantics, and graceful degradation when (not if) things fail.

This chapter covers messaging platform design as Staff Engineers practice it: with deep understanding of the consistency-availability trade-offs unique to messaging, awareness of the ordering problems that seem simple but aren't, and judgment about when to sacrifice guarantees for user experience.

**The Staff Engineer's First Law of Messaging**: Users care about three things in order: (1) Did my message get delivered? (2) Did it arrive quickly? (3) Is the conversation in the right order? Everything elseâ€”read receipts, typing indicators, reactionsâ€”is polish. Get the fundamentals wrong, and no amount of features saves you.

---

## Quick Visual: Messaging Platform at a Glance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MESSAGING PLATFORM: THE STAFF ENGINEER VIEW                â”‚
â”‚                                                                             â”‚
â”‚   WRONG Framing: "Send messages between users"                              â”‚
â”‚   RIGHT Framing: "Deliver messages reliably with ordering guarantees,       â”‚
â”‚                   sub-second latency, and graceful offline handling"        â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Before designing, understand:                                      â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  1. 1:1 messaging or group messaging or both?                       â”‚   â”‚
â”‚   â”‚  2. What ordering guarantees? (Per-sender? Per-conversation?)       â”‚   â”‚
â”‚   â”‚  3. How long can users be offline? (Minutes? Days? Months?)         â”‚   â”‚
â”‚   â”‚  4. What's the message size distribution? (Text? Media? Files?)     â”‚   â”‚
â”‚   â”‚  5. Synchronous delivery (online) or async (offline)?               â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   THE UNCOMFORTABLE TRUTH:                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Perfect ordering + perfect availability + low latency is           â”‚   â”‚
â”‚   â”‚  impossible across regions. You must choose which to sacrifice.     â”‚   â”‚
â”‚   â”‚  Most messaging apps sacrifice global ordering for availability     â”‚   â”‚
â”‚   â”‚  and use sender-ordering + local clocks for "good enough" UX.       â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## L5 vs L6 Messaging Platform Decisions

| Scenario | L5 Approach | L6 Approach |
|----------|-------------|-------------|
| **Message ordering** | "Use a single message queue for total ordering" | "Total ordering doesn't scale. Use per-conversation ordering with Lamport timestamps. Users only see their own conversation anyway." |
| **Delivery guarantees** | "Use exactly-once delivery with 2PC" | "Exactly-once is expensive. Use at-least-once with idempotent receivers. Duplicate detection at client is cheaper than prevention." |
| **Offline message storage** | "Store all messages forever in hot storage" | "Hot storage for 30 days, warm for 1 year, cold archive beyond. 99% of reads are last 24 hours." |
| **Group messaging** | "Fan-out on write for fast reads" | "Fan-out on read for small groups, fan-out on write for large groups. Threshold at ~50 members." |
| **Presence (online status)** | "Real-time presence with WebSocket heartbeats" | "Presence is eventually consistent. 30-second staleness is acceptable. Don't make presence a scaling bottleneck." |

**Key Difference**: L6 engineers recognize that messaging systems have fundamentally different requirements than traditional request-response systems. They design for the unique challenges of long-lived connections, offline users, and conversation-local consistency.

---

# Part 1: Foundations â€” What a Messaging Platform Is and Why It Exists

## What Is a Messaging Platform?

A messaging platform enables asynchronous communication between users through discrete messages. Unlike email (store-and-forward, minutes-to-hours delivery) or phone calls (synchronous, real-time), messaging platforms occupy a middle ground: near-real-time when users are online, reliably stored when they're offline.

### The Simplest Mental Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MESSAGING: THE POSTAL SYSTEM ANALOGY                       â”‚
â”‚                                                                             â”‚
â”‚   Think of messaging as a modern postal system:                             â”‚
â”‚                                                                             â”‚
â”‚   SENDER writes a letter (message)                                          â”‚
â”‚   â†“                                                                         â”‚
â”‚   POST OFFICE (message service) receives it                                 â”‚
â”‚   â†“                                                                         â”‚
â”‚   If RECIPIENT is home (online): Deliver immediately                        â”‚
â”‚   If RECIPIENT is away (offline): Hold at post office, deliver on return   â”‚
â”‚   â†“                                                                         â”‚
â”‚   RECIPIENT receives letter, sends acknowledgment (read receipt)            â”‚
â”‚                                                                             â”‚
â”‚   COMPLICATIONS AT SCALE:                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  What if there are millions of post offices (distributed system)?  â”‚   â”‚
â”‚   â”‚  â†’ Need coordination for which office holds which letters          â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  What if recipient has multiple homes (multiple devices)?          â”‚   â”‚
â”‚   â”‚  â†’ Need to track delivery to ALL devices, not just one             â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  What if letters arrive out of order?                               â”‚   â”‚
â”‚   â”‚  â†’ Need sequence numbers to reorder at recipient                   â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  What if sender sends to 1000 recipients (group chat)?             â”‚   â”‚
â”‚   â”‚  â†’ Need efficient fan-out, not 1000 individual deliveries          â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Why Messaging Platforms Exist

### 1. Asynchronous Communication at Human Speed

Email is too slow for conversations. Phone calls require both parties simultaneously. Messaging bridges the gap:

```
Scenario: Coordinating dinner plans

EMAIL:
T+0min:   Alice sends "Dinner at 7?"
T+5min:   Bob checks email, replies "Sure, where?"
T+12min:  Alice checks email, replies "That Italian place"
T+20min:  Bob confirms
TOTAL: 20+ minutes for simple coordination

PHONE:
T+0min:   Alice calls Bob
          Bob doesn't answer (in meeting)
T+30min:  Bob calls back
          Alice doesn't answer (now in meeting)
T+60min:  Finally connect, 2-minute conversation
TOTAL: 60+ minutes wall clock time

MESSAGING:
T+0min:   Alice sends "Dinner at 7?"
T+0min:   Bob sees notification, replies "Sure, where?"
T+1min:   Alice replies "That Italian place"
T+1min:   Bob sends ğŸ‘
TOTAL: 1-2 minutes, no synchronization required
```

### 2. Persistent Conversation History

Unlike phone calls, messages create a record:

```
Benefits:
â€¢ Search old conversations for decisions/agreements
â€¢ Onboard new group members with context
â€¢ Reference shared links, files, addresses
â€¢ Legal/compliance record-keeping
â€¢ Memory augmentation ("What did we decide last week?")
```

### 3. Multi-Device, Multi-Platform Access

Modern messaging works across all devices simultaneously:

```
User's devices:
â”œâ”€â”€ Phone (primary, always connected)
â”œâ”€â”€ Tablet (occasional use)
â”œâ”€â”€ Laptop (work hours)
â””â”€â”€ Desktop (home)

Message arrives:
â†’ Delivered to ALL devices
â†’ Read status synced across devices
â†’ User can respond from any device
â†’ Conversation continues seamlessly
```

### 4. Rich Communication Beyond Text

Modern platforms support:

```
Content types:
â”œâ”€â”€ Text (with emoji, formatting)
â”œâ”€â”€ Images (with compression, thumbnails)
â”œâ”€â”€ Videos (with streaming, transcoding)
â”œâ”€â”€ Voice messages (with waveform preview)
â”œâ”€â”€ Files/documents
â”œâ”€â”€ Location sharing
â”œâ”€â”€ Contact cards
â”œâ”€â”€ Polls and interactive elements
â””â”€â”€ Reactions and replies
```

## What Happens If a Messaging Platform Does NOT Exist (or Fails)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  MESSAGING PLATFORM FAILURES                                â”‚
â”‚                                                                             â”‚
â”‚   FAILURE MODE 1: MESSAGE LOSS                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  User sends critical message â†’ Message disappears â†’ No notification â”‚   â”‚
â”‚   â”‚  â†’ User believes message was delivered â†’ Relationship damaged       â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Real example: "I told you I couldn't make it!" / "I never got it" â”‚   â”‚
â”‚   â”‚  This is THE cardinal sin of messaging platforms                    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   FAILURE MODE 2: MESSAGE REORDERING                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Messages arrive out of order â†’ Conversation is confusing          â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Alice: "Should we go to dinner?"                                   â”‚   â”‚
â”‚   â”‚  Bob: "Yes"                                                         â”‚   â”‚
â”‚   â”‚  Alice: "Or maybe just drinks?"                                     â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Bob sees: "Or maybe just drinks?" then "Should we go to dinner?"  â”‚   â”‚
â”‚   â”‚  Bob's "Yes" now makes no sense                                    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   FAILURE MODE 3: DUPLICATE MESSAGES                                        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Same message delivered multiple times â†’ User confusion/annoyance  â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Less severe than loss, but still breaks trust                     â”‚   â”‚
â”‚   â”‚  "Why did you send that 5 times?"                                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   FAILURE MODE 4: PRESENCE LIES                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Shows "online" when offline or vice versa                         â”‚   â”‚
â”‚   â”‚  â†’ Social friction ("Why aren't you responding? You're online!")   â”‚   â”‚
â”‚   â”‚                                                                     â”‚   â”‚
â”‚   â”‚  Staff insight: This is why many apps show "last seen" instead     â”‚   â”‚
â”‚   â”‚  of "online now"â€”it's more forgiving of staleness                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   FAILURE MODE 5: SYNC DIVERGENCE                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  Different devices show different message history                  â”‚   â”‚
â”‚   â”‚  â†’ User confusion about what was actually said                     â”‚   â”‚
â”‚   â”‚  â†’ "I can see it on my phone but not my laptop"                    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 2: Functional Requirements

## Core Use Cases

### 1. Send a Message (1:1 Conversation)

```
Use Case: Alice sends "Hello" to Bob

Input: 
  - Sender: Alice
  - Recipient: Bob  
  - Content: "Hello"
  - Timestamp: 2024-01-15T10:30:00Z

Process:
  1. Authenticate Alice
  2. Validate message (size, content policy)
  3. Generate message ID (globally unique)
  4. Store message durably
  5. If Bob online: Push to Bob's device(s)
  6. If Bob offline: Queue for later delivery
  7. Acknowledge to Alice (single checkmark)
  8. When Bob receives: Update to delivered (double checkmark)
  9. When Bob reads: Update to read (blue checkmarks)

Output:
  - Message ID for reference
  - Delivery status updates
```

### 2. Send a Message (Group Conversation)

```
Use Case: Alice sends "Meeting at 3" to group "Project Team" (50 members)

Input:
  - Sender: Alice
  - Group: project_team_123
  - Content: "Meeting at 3"

Process:
  1. Authenticate Alice
  2. Verify Alice is group member
  3. Generate message ID
  4. Store message (once, not 50 times)
  5. Fan-out delivery to 50 members
  6. Track per-member delivery status

Complications:
  - Some members online, some offline
  - Some members have notifications muted
  - Some members blocked Alice
  - Members in different timezones
```

### 3. Receive Messages (Real-time)

```
Use Case: Bob receives messages while app is open

Mechanism: Long-lived WebSocket connection

Process:
  1. Bob's device establishes WebSocket to message gateway
  2. Gateway registers Bob's connection in presence service
  3. When message arrives for Bob:
     a. Look up Bob's active connections
     b. Push message through WebSocket
     c. Wait for ACK from device
     d. Mark message as delivered
  4. If connection drops: Re-establish with resume token

Latency target: < 500ms from send to receive
```

### 4. Receive Messages (Offline Sync)

```
Use Case: Bob opens app after 3 days offline

Process:
  1. Bob's device connects with last_sync_timestamp
  2. Server queries: "All messages for Bob since timestamp"
  3. Return messages in batches (pagination)
  4. For each conversation: Return messages in order
  5. Client merges with local state
  6. Update sync checkpoint

Complications:
  - Bob might have 10,000 unread messages
  - Some messages might be deleted by sender
  - Media might need re-download
  - Group memberships might have changed
```

### 5. Typing Indicators

```
Use Case: Alice sees "Bob is typing..."

Mechanism: Ephemeral, unreliable signaling

Process:
  1. Bob types â†’ Client sends "typing" signal
  2. Signal routed to Alice (if online)
  3. Alice displays indicator
  4. Indicator auto-expires after 5 seconds
  5. No persistence, no retry, no guarantee

Staff insight: Typing indicators are cosmetic.
- Lost typing indicators = no harm
- Stale typing indicators = minor annoyance
- Don't optimize for this; focus on message delivery
```

### 6. Read Receipts

```
Use Case: Alice sees that Bob read her message

Process:
  1. Bob's device renders message on screen
  2. Client sends read_receipt(message_id, timestamp)
  3. Receipt stored and forwarded to Alice
  4. Alice's UI updates to show "read"

Privacy considerations:
  - Some users disable read receipts
  - Group read receipts: Show count, not names
  - Read != comprehended (user may have scrolled past)
```

### 7. Message Editing and Deletion

```
Use Case: Alice edits typo / deletes embarrassing message

Edit process:
  1. Alice sends edit(message_id, new_content)
  2. Validate: Alice is author, within edit window
  3. Update message content
  4. Push edit to all recipients
  5. UI shows "edited" indicator

Delete process:
  1. Alice sends delete(message_id)
  2. Options:
     a. Delete for self only (just hide in Alice's view)
     b. Delete for everyone (remove content, show "deleted")
  3. Delete window: Typically 1-48 hours

Staff insight: "Delete for everyone" is contentious.
- Recipient may have already seen/screenshotted
- Creates strange conversation gaps
- Some platforms show "Alice deleted a message"
```

## Read Paths

```
// Get conversation history (hot path)
FUNCTION get_messages(conversation_id, user_id, cursor, limit):
    // Verify access
    IF NOT is_member(user_id, conversation_id):
        RETURN ERROR("Access denied")
    
    // Fetch messages
    messages = query_messages(
        conversation_id, 
        before_cursor=cursor, 
        limit=limit
    )
    
    // Fetch media URLs (signed, time-limited)
    FOR message IN messages:
        IF message.has_media:
            message.media_url = generate_signed_url(message.media_id)
    
    RETURN {
        messages: messages,
        next_cursor: messages.last().id,
        has_more: count > limit
    }

// Get conversation list
FUNCTION get_conversations(user_id, cursor, limit):
    conversations = query_user_conversations(
        user_id,
        order_by=last_message_time DESC,
        limit=limit
    )
    
    // Include snippet of last message
    FOR conv IN conversations:
        conv.last_message = get_last_message(conv.id)
        conv.unread_count = get_unread_count(conv.id, user_id)
    
    RETURN conversations
```

## Write Paths

```
// Send message (critical path)
FUNCTION send_message(sender_id, conversation_id, content, media):
    // Validate
    IF NOT is_member(sender_id, conversation_id):
        RETURN ERROR("Not a member")
    
    IF length(content) > MAX_MESSAGE_SIZE:
        RETURN ERROR("Message too long")
    
    // Generate IDs
    message_id = generate_snowflake_id()
    
    // Handle media upload (async)
    IF media:
        media_id = upload_media_async(media)
    
    // Create message
    message = {
        id: message_id,
        conversation_id: conversation_id,
        sender_id: sender_id,
        content: content,
        media_id: media_id,
        created_at: now(),
        sequence: get_next_sequence(conversation_id)
    }
    
    // Persist (must succeed before ACK)
    store_message(message)
    
    // Update conversation metadata
    update_conversation_last_message(conversation_id, message)
    
    // Trigger delivery (async)
    enqueue_delivery(message)
    
    RETURN {
        message_id: message_id,
        status: "sent",
        timestamp: message.created_at
    }
```

## Control / Admin Paths

```
// Create group conversation
FUNCTION create_group(creator_id, name, member_ids):
    IF len(member_ids) > MAX_GROUP_SIZE:
        RETURN ERROR("Group too large")
    
    conversation_id = generate_id()
    
    create_conversation({
        id: conversation_id,
        type: "group",
        name: name,
        created_by: creator_id,
        created_at: now()
    })
    
    // Add members (including creator)
    FOR member_id IN [creator_id] + member_ids:
        add_member(conversation_id, member_id, role="member")
    
    set_admin(conversation_id, creator_id)
    
    // Send system message
    send_system_message(conversation_id, "Group created")
    
    RETURN conversation_id

// Block user
FUNCTION block_user(blocker_id, blocked_id):
    create_block(blocker_id, blocked_id)
    
    // Prevent future messages
    // Hide existing conversations (don't delete)
    // Remove from each other's contact lists
    
    RETURN SUCCESS

// Report content
FUNCTION report_message(reporter_id, message_id, reason):
    message = get_message(message_id)
    
    create_report({
        reporter_id: reporter_id,
        message_id: message_id,
        conversation_id: message.conversation_id,
        content_snapshot: message.content,
        reason: reason,
        created_at: now()
    })
    
    // Trigger async review workflow
    enqueue_moderation_review(report)
    
    RETURN SUCCESS
```

## Edge Cases

### Edge Case 1: Sending to Blocked User

```
Alice blocked Bob. Bob tries to send message to Alice.

Options:
A) Error: "Cannot send to this user" (reveals block)
B) Silent success: Message appears sent but never delivered
C) Delayed failure: "Message could not be delivered"

Staff approach: Option B (silent success)
- Prevents harassment escalation
- Blocks are private information
- Bob sees sent status, Alice never sees message
```

### Edge Case 2: Group Message to Mixed Online/Offline

```
Group has 100 members:
- 40 online
- 30 offline (will be back today)
- 20 offline (haven't opened app in months)
- 10 have uninstalled app

Message delivery:
1. Immediate push to 40 online members
2. Store for 30 recently-active offline members
3. Store but don't push-notify 20 inactive members
4. Detect uninstalled via push failure, mark for cleanup
```

### Edge Case 3: Message During Network Partition

```
Alice sends message, but network fails before ACK

Client behavior:
1. Show "sending..." indicator
2. Retry with exponential backoff
3. If server ACKs: Update to "sent"
4. If server rejects as duplicate: Already delivered, update status
5. If timeout after N retries: Show "failed to send, tap to retry"

Server behavior:
- Use client-generated message ID for idempotency
- If same ID received twice, return success (already stored)
```

### Edge Case 4: Device Clock Far in Future

```
Alice's phone clock is 1 year ahead.
Message timestamp: 2025-01-15 (actual date: 2024-01-15)

Problems:
- Message sorts to wrong position
- "Last seen" shows future date
- Message expiry confused

Staff approach:
- Server assigns authoritative timestamp
- Client timestamp used only for offline sorting
- On sync, server timestamp wins
```

## What Is Intentionally OUT of Scope

| Excluded | Why |
|----------|-----|
| Voice/video calling | Different infrastructure (WebRTC, media servers) |
| Stories/status updates | Different content model (broadcast, ephemeral) |
| End-to-end encryption | Massive complexity, covered separately |
| Content moderation ML | Separate system, messaging just provides data |
| Payment/commerce | Different compliance, security requirements |
| Bots/integrations platform | API layer on top of core messaging |

---

# Part 3: Non-Functional Requirements

## Latency Expectations

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LATENCY REQUIREMENTS                                     â”‚
â”‚                                                                             â”‚
â”‚   MESSAGE SEND (User presses send â†’ sees "sent"):                           â”‚
â”‚   â€¢ P50: < 100ms (feels instant)                                            â”‚
â”‚   â€¢ P95: < 300ms (acceptable)                                               â”‚
â”‚   â€¢ P99: < 1000ms (user notices but tolerates)                              â”‚
â”‚   â€¢ > 2000ms: User retries, potential duplicate                             â”‚
â”‚                                                                             â”‚
â”‚   MESSAGE DELIVERY (Send â†’ recipient sees):                                 â”‚
â”‚   â€¢ P50: < 500ms (real-time feel)                                           â”‚
â”‚   â€¢ P95: < 2000ms (still responsive)                                        â”‚
â”‚   â€¢ P99: < 5000ms (degraded but working)                                    â”‚
â”‚                                                                             â”‚
â”‚   CONVERSATION LOAD (Open chat â†’ see messages):                             â”‚
â”‚   â€¢ P50: < 200ms (instant)                                                  â”‚
â”‚   â€¢ P99: < 500ms (smooth)                                                   â”‚
â”‚                                                                             â”‚
â”‚   TYPING INDICATORS:                                                        â”‚
â”‚   â€¢ < 300ms to appear (feels real-time)                                     â”‚
â”‚   â€¢ Can drop indicators under load (cosmetic)                               â”‚
â”‚                                                                             â”‚
â”‚   STAFF INSIGHT:                                                            â”‚
â”‚   Message send latency is sacredâ€”it's the user's signal that the           â”‚
â”‚   system is working. Everything else can degrade.                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Availability Expectations

```
Target: 99.99% availability (52 minutes downtime/year)

What "available" means for messaging:
1. Users can send messages (even if delivery is delayed)
2. Users can read existing messages
3. New messages eventually arrive

Acceptable degradations during partial outage:
â€¢ Delayed delivery (minutes, not hours)
â€¢ Missing typing indicators
â€¢ Stale presence information
â€¢ Slow media upload/download
â€¢ Missing read receipts

Unacceptable during any outage:
â€¢ Message loss
â€¢ Message corruption
â€¢ Unauthorized access to messages
```

## Consistency Needs

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONSISTENCY MODEL                                        â”‚
â”‚                                                                             â”‚
â”‚   STRONG CONSISTENCY REQUIRED:                                              â”‚
â”‚   â€¢ Message ordering within conversation (per-sender at minimum)            â”‚
â”‚   â€¢ Membership changes (who can see which messages)                         â”‚
â”‚   â€¢ Block/unblock (must take effect immediately)                            â”‚
â”‚                                                                             â”‚
â”‚   EVENTUAL CONSISTENCY ACCEPTABLE:                                          â”‚
â”‚   â€¢ Read receipts (can lag by seconds)                                      â”‚
â”‚   â€¢ Typing indicators (ephemeral anyway)                                    â”‚
â”‚   â€¢ Presence status (30-second staleness OK)                                â”‚
â”‚   â€¢ Unread counts (can lag)                                                 â”‚
â”‚   â€¢ Cross-device sync (seconds of delay OK)                                 â”‚
â”‚                                                                             â”‚
â”‚   THE FUNDAMENTAL TRADE-OFF:                                                â”‚
â”‚   Global total ordering of messages is impossible at scale.                â”‚
â”‚   We use conversation-local ordering with Lamport-style timestamps.        â”‚
â”‚                                                                             â”‚
â”‚   Within single conversation: Messages appear in consistent order           â”‚
â”‚   Across conversations: No global ordering guarantee                        â”‚
â”‚   Across regions: May see messages from other region with lag               â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Durability

```
Message durability requirements:
â€¢ Messages must NEVER be lost after ACK to sender
â€¢ Durability: 99.999999999% (11 nines) for message content
â€¢ This requires synchronous replication before ACK

Strategy:
1. Write to primary storage (with sync replication)
2. Only ACK to sender after durable write confirmed
3. Async replication to secondary regions for disaster recovery

Media durability:
â€¢ Same 11 nines for media files
â€¢ Store in blob storage with cross-region replication
â€¢ Keep metadata even if media expires (show "media expired")
```

## Correctness vs User Experience Trade-offs

```
CORRECTNESS:                           USER EXPERIENCE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Wait for full sync before showing      Show cached data immediately,
conversation                           sync in background
                                       â†’ Choose UX (stale > slow)

Block all operations during            Allow reads, queue writes
network issues                         â†’ Choose UX (partial > nothing)

Show precise message timestamps        Show "2 minutes ago"
(10:42:37.123)                         â†’ Choose UX (human-readable)

Enforce strict ordering                Allow out-of-order display,
(wait for missing messages)            reorder when gap fills
                                       â†’ Depends: strict for 1:1,
                                         relaxed for busy groups

Verify read receipt delivery           Best-effort read receipts
before showing "read"                  â†’ Choose UX (fast > accurate)
```

## Security Implications

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SECURITY REQUIREMENTS                                    â”‚
â”‚                                                                             â”‚
â”‚   CONFIDENTIALITY:                                                          â”‚
â”‚   â€¢ Messages visible only to conversation participants                      â”‚
â”‚   â€¢ No leakage through errors, logs, or side channels                       â”‚
â”‚   â€¢ Encryption in transit (TLS)                                             â”‚
â”‚   â€¢ Encryption at rest (storage encryption)                                 â”‚
â”‚   â€¢ Optional: End-to-end encryption (E2EE)                                  â”‚
â”‚                                                                             â”‚
â”‚   INTEGRITY:                                                                â”‚
â”‚   â€¢ Messages cannot be modified in transit                                  â”‚
â”‚   â€¢ Message history cannot be tampered with                                 â”‚
â”‚   â€¢ Sender identity is authentic                                            â”‚
â”‚                                                                             â”‚
â”‚   ACCESS CONTROL:                                                           â”‚
â”‚   â€¢ Only authenticated users can send/receive                               â”‚
â”‚   â€¢ Group membership enforced                                               â”‚
â”‚   â€¢ Block lists enforced                                                    â”‚
â”‚   â€¢ Admin actions properly authorized                                       â”‚
â”‚                                                                             â”‚
â”‚   STAFF INSIGHT:                                                            â”‚
â”‚   Messaging has strict privacy expectations. A single bug that              â”‚
â”‚   leaks messages to wrong recipient is a PR disaster and potential          â”‚
â”‚   legal liability. Defense in depth is essential.                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 4: Scale & Load Modeling

## User Scale

```
Reference: WhatsApp-scale messaging platform

Users:
â€¢ Total registered users: 2 billion
â€¢ Daily active users (DAU): 500 million
â€¢ Monthly active users (MAU): 1.5 billion
â€¢ Concurrent connections: 100 million (peak)

Conversations:
â€¢ Average conversations per user: 20
â€¢ Total 1:1 conversations: 10 billion
â€¢ Total group conversations: 500 million
â€¢ Average group size: 10 members
â€¢ Large groups (>100 members): 5 million
```

## Message Volume

```
Messages per day:
â€¢ Total messages: 100 billion/day
â€¢ Average per DAU: 200 messages sent+received
â€¢ Peak hour: 10 billion messages (10x average)

Message breakdown:
â€¢ Text only: 70% (avg 100 bytes)
â€¢ Images: 20% (avg 200KB compressed)
â€¢ Video: 5% (avg 5MB)
â€¢ Voice: 3% (avg 100KB)
â€¢ Other (files, location): 2%
```

## QPS Calculations

```
STEADY STATE:
Messages: 100B/day Ã· 86,400 sec = 1.15M messages/sec
Peak: ~10M messages/sec

Per message, multiple operations:
â€¢ 1 write (store message)
â€¢ N reads (N recipients, plus sender confirmation)
â€¢ 1-3 push notifications
â€¢ 1 unread count update

Effective operations:
â€¢ Write QPS: 10M/sec (peak)
â€¢ Read QPS: 50M/sec (peak) - 5 reads per message avg
â€¢ Push QPS: 30M/sec (peak)

CONNECTION MANAGEMENT:
â€¢ 100M concurrent WebSocket connections
â€¢ Each connection: heartbeat every 30 seconds
â€¢ Heartbeat QPS: 3.3M/sec
```

## Storage Requirements

```
MESSAGE STORAGE:
Daily text: 70B messages Ã— 100 bytes = 7TB/day
Daily images: 20B Ã— 200KB = 4PB/day
Daily video: 5B Ã— 5MB = 25PB/day
Daily total: ~30PB/day

With retention:
â€¢ Hot storage (30 days): 900PB
â€¢ Warm storage (1 year): 10EB
â€¢ Cold storage (archive): âˆ

METADATA STORAGE:
â€¢ Per message: ~500 bytes metadata
â€¢ 100B messages Ã— 500 bytes = 50TB/day
â€¢ Indexes: 2x metadata = 100TB/day

CONVERSATION METADATA:
â€¢ 10B conversations Ã— 1KB = 10TB
â€¢ Relatively static, heavily cached
```

## Burst Behavior

```
PREDICTABLE BURSTS:
â€¢ New Year's midnight: 100x normal (by timezone)
â€¢ Major sporting events: 10-50x
â€¢ Celebrity deaths/announcements: 20x
â€¢ Religious holidays: 5-10x

UNPREDICTABLE BURSTS:
â€¢ Breaking news: 10-20x
â€¢ Viral content: Localized 50x spikes
â€¢ Platform outage recovery: 10x (message backlog)

SCALING IMPLICATIONS:
â€¢ Must handle 100x burst without data loss
â€¢ Acceptable to delay delivery during extreme bursts
â€¢ Must shed load gracefully (typing indicators first)
```

## What Breaks First

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SCALING BOTTLENECKS (In Order)                           â”‚
â”‚                                                                             â”‚
â”‚   1. CONNECTION SERVERS (First to break)                                    â”‚
â”‚      â€¢ 100M WebSocket connections = massive memory                          â”‚
â”‚      â€¢ Connection storms after outage                                       â”‚
â”‚      â€¢ Solution: Horizontal scaling, connection limits per user             â”‚
â”‚                                                                             â”‚
â”‚   2. MESSAGE FANOUT FOR LARGE GROUPS                                        â”‚
â”‚      â€¢ 1000-member group = 1000 deliveries per message                      â”‚
â”‚      â€¢ Active 1000-member group = 1M deliveries/day                        â”‚
â”‚      â€¢ Solution: Read fanout for large groups, lazy delivery               â”‚
â”‚                                                                             â”‚
â”‚   3. PRESENCE SERVICE                                                       â”‚
â”‚      â€¢ 100M users each updating presence                                    â”‚
â”‚      â€¢ Naive approach: NÂ² presence checks                                   â”‚
â”‚      â€¢ Solution: Approximate presence, subscription limits                  â”‚
â”‚                                                                             â”‚
â”‚   4. HOT CONVERSATIONS                                                      â”‚
â”‚      â€¢ Celebrity/influencer conversations                                   â”‚
â”‚      â€¢ Millions of fans messaging same person                               â”‚
â”‚      â€¢ Solution: Queue and rate-limit inbound, special handling             â”‚
â”‚                                                                             â”‚
â”‚   5. MEDIA STORAGE COSTS                                                    â”‚
â”‚      â€¢ 30PB/day is $$$                                                      â”‚
â”‚      â€¢ Solution: Aggressive compression, expiry, tiered storage             â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 5: High-Level Architecture

## Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MESSAGING PLATFORM ARCHITECTURE                          â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                         CLIENTS                                     â”‚   â”‚
â”‚   â”‚         Mobile Apps  â”‚  Web App  â”‚  Desktop Apps                    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      EDGE / API LAYER                               â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚   â”‚  â”‚   API       â”‚  â”‚  WebSocket  â”‚  â”‚   Push      â”‚                  â”‚   â”‚
â”‚   â”‚  â”‚   Gateway   â”‚  â”‚  Gateway    â”‚  â”‚   Gateway   â”‚                  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      CORE SERVICES                                  â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚   â”‚  â”‚   Message   â”‚  â”‚  Delivery   â”‚  â”‚  Presence   â”‚                  â”‚   â”‚
â”‚   â”‚  â”‚   Service   â”‚  â”‚  Service    â”‚  â”‚  Service    â”‚                  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚   â”‚  â”‚   Sync      â”‚  â”‚  Group      â”‚  â”‚  User       â”‚                  â”‚   â”‚
â”‚   â”‚  â”‚   Service   â”‚  â”‚  Service    â”‚  â”‚  Service    â”‚                  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                â”‚                                            â”‚
â”‚                                â–¼                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                      DATA LAYER                                     â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚   â”‚  â”‚   Message   â”‚  â”‚   User      â”‚  â”‚   Media     â”‚                  â”‚   â”‚
â”‚   â”‚  â”‚   Store     â”‚  â”‚   Store     â”‚  â”‚   Store     â”‚                  â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚   â”‚
â”‚   â”‚  â”‚   Cache     â”‚  â”‚   Message   â”‚                                   â”‚   â”‚
â”‚   â”‚  â”‚   Layer     â”‚  â”‚   Queue     â”‚                                   â”‚   â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Component Responsibilities

### API Gateway
- HTTP REST endpoints for non-real-time operations
- Authentication and authorization
- Rate limiting
- Request routing

### WebSocket Gateway
- Maintains long-lived connections with clients
- Routes real-time messages to correct connections
- Handles connection lifecycle (connect, heartbeat, disconnect)
- Stateful: knows which users are connected

### Push Gateway
- Sends push notifications to offline users
- Integrates with APNs (iOS), FCM (Android)
- Batches and prioritizes notifications
- Handles token management and failures

### Message Service
- Core message CRUD operations
- Message validation and storage
- Sequence number assignment
- Handles edits and deletes

### Delivery Service
- Routes messages to recipients
- Manages online vs offline delivery
- Tracks delivery and read status
- Handles retries and failures

### Presence Service
- Tracks online/offline status
- Manages "last seen" timestamps
- Subscription model for presence updates
- Handles privacy settings

### Sync Service
- Provides message history for offline clients
- Handles multi-device synchronization
- Manages sync cursors and checkpoints
- Resolves conflicts

### Group Service
- Group CRUD operations
- Membership management
- Admin functions
- Group metadata

### User Service
- User profiles and settings
- Contact lists
- Block lists
- Preferences

## Stateless vs Stateful Decisions

```
STATELESS SERVICES:
â€¢ Message Service: Processes requests, stores in DB
â€¢ Group Service: CRUD operations, DB-backed
â€¢ User Service: Profile operations, DB-backed
â€¢ Sync Service: Query and return, no local state

STATEFUL SERVICES:
â€¢ WebSocket Gateway: Holds active connections
  - State: user_id â†’ connection mapping
  - Failure: connections drop, clients reconnect
  
â€¢ Presence Service: Tracks who's online
  - State: online users, last activity
  - Failure: presence becomes stale, self-heals
  
â€¢ Delivery Service: In-flight message tracking
  - State: pending deliveries
  - Failure: re-query from message store, retry

STAFF INSIGHT:
Stateful services are harder to scale and recover.
We minimize state and make it reconstructible.
WebSocket state is client-recoverable (reconnect).
Presence state is ephemeral (rebuild on restart).
Delivery state is checkpointed (resume from checkpoint).
```

## Data Flow: Send Message

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEND MESSAGE FLOW                                        â”‚
â”‚                                                                             â”‚
â”‚   1. SENDER DEVICE                                                          â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   2. API GATEWAY                                                            â”‚
â”‚      â€¢ Authenticate request                                                 â”‚
â”‚      â€¢ Rate limit check                                                     â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   3. MESSAGE SERVICE                                                        â”‚
â”‚      â€¢ Validate message                                                     â”‚
â”‚      â€¢ Generate message ID + sequence number                                â”‚
â”‚      â€¢ Store message in MESSAGE STORE (sync write)                          â”‚
â”‚      â€¢ Return ACK to sender                                                 â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   4. DELIVERY SERVICE (async)                                               â”‚
â”‚      â€¢ Look up conversation members                                         â”‚
â”‚      â€¢ For each recipient:                                                  â”‚
â”‚        â”‚                                                                    â”‚
â”‚        â”œâ”€â–º ONLINE: Query PRESENCE SERVICE                                   â”‚
â”‚        â”‚     â””â”€â–º Send via WEBSOCKET GATEWAY                                 â”‚
â”‚        â”‚                                                                    â”‚
â”‚        â””â”€â–º OFFLINE: Push notification via PUSH GATEWAY                      â”‚
â”‚                                                                             â”‚
â”‚   5. RECIPIENT DEVICE(S)                                                    â”‚
â”‚      â€¢ Receive message via WebSocket or push                                â”‚
â”‚      â€¢ Send delivery ACK                                                    â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   6. DELIVERY SERVICE                                                       â”‚
â”‚      â€¢ Update delivery status                                               â”‚
â”‚      â€¢ Notify sender of delivery                                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow: Receive Messages (Sync)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SYNC MESSAGE FLOW                                        â”‚
â”‚                                                                             â”‚
â”‚   1. CLIENT connects after being offline                                    â”‚
â”‚      â€¢ Sends: last_sync_id = "msg_12345"                                    â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   2. SYNC SERVICE                                                           â”‚
â”‚      â€¢ Query: all messages for user since msg_12345                         â”‚
â”‚      â€¢ Group by conversation                                                â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   3. MESSAGE STORE                                                          â”‚
â”‚      â€¢ Index: (user_id, message_id) for inbox                               â”‚
â”‚      â€¢ Returns paginated results                                            â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   4. SYNC SERVICE                                                           â”‚
â”‚      â€¢ Enrich with conversation metadata                                    â”‚
â”‚      â€¢ Fetch sender info from cache                                         â”‚
â”‚      â€¢ Return to client with new sync cursor                                â”‚
â”‚      â”‚                                                                      â”‚
â”‚      â–¼                                                                      â”‚
â”‚   5. CLIENT                                                                 â”‚
â”‚      â€¢ Merge with local message store                                       â”‚
â”‚      â€¢ Update UI                                                            â”‚
â”‚      â€¢ Store new sync cursor                                                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 6: Deep Component Design

## WebSocket Gateway

### Purpose
Maintains persistent connections with clients for real-time message delivery.

### Internal Data Structures

```
// Per-server connection registry
ConnectionRegistry:
    connections: Map<connection_id, Connection>
    user_connections: Map<user_id, Set<connection_id>>
    
Connection:
    connection_id: string
    user_id: string
    device_id: string
    socket: WebSocket
    connected_at: timestamp
    last_activity: timestamp
    resume_token: string

// Distributed user location
UserLocationService:
    user_locations: Map<user_id, List<ServerAddress>>
    // Backed by Redis with TTL
```

### Connection Lifecycle

```
CONNECT:
1. Client initiates WebSocket handshake
2. Gateway validates auth token
3. Generate connection_id and resume_token
4. Register in local ConnectionRegistry
5. Publish to UserLocationService: "user X on server Y"
6. Send connection ACK with resume_token

HEARTBEAT:
1. Client sends ping every 30 seconds
2. Server responds pong
3. Update last_activity timestamp
4. If no heartbeat for 90 seconds: consider dead

DISCONNECT:
1. Client closes or connection times out
2. Remove from local ConnectionRegistry
3. Remove from UserLocationService
4. Keep resume_token valid for 5 minutes (reconnect window)

RESUME:
1. Client reconnects with resume_token
2. Validate token (not expired, correct user)
3. Replay any messages since disconnect
4. Issue new resume_token
```

### Message Routing

```
FUNCTION route_message_to_user(user_id, message):
    // Find all connections for this user
    locations = UserLocationService.get(user_id)
    
    IF locations is empty:
        RETURN NOT_CONNECTED
    
    FOR location IN locations:
        IF location.server == self:
            // Local delivery
            connections = ConnectionRegistry.get_user_connections(user_id)
            FOR conn IN connections:
                conn.socket.send(message)
        ELSE:
            // Remote delivery via internal RPC
            send_to_server(location.server, user_id, message)
    
    RETURN DELIVERED
```

### Failure Behavior

```
SERVER CRASH:
â€¢ All connections on that server drop
â€¢ Clients detect via missed heartbeat (30-90 seconds)
â€¢ Clients reconnect to different server
â€¢ Messages during gap: delivered on reconnect via sync

NETWORK PARTITION:
â€¢ Connections appear alive but unresponsive
â€¢ Messages queue up, eventually timeout
â€¢ Client-side timeout triggers reconnect

OVERLOAD:
â€¢ New connections rejected with backoff header
â€¢ Existing connections maintained
â€¢ Message delivery prioritized over new connections
```

### Why Simpler Alternatives Fail

```
Alternative: HTTP Long Polling
Problems:
â€¢ Higher latency (connection setup per message)
â€¢ More server load (new connection per poll)
â€¢ Harder to maintain ordering guarantees
â€¢ More complex client-side logic

Alternative: Server-Sent Events (SSE)
Problems:
â€¢ Unidirectional (serverâ†’client only)
â€¢ Need separate channel for clientâ†’server
â€¢ Less efficient for bidirectional chat

Alternative: Single WebSocket server
Problems:
â€¢ Single point of failure
â€¢ Cannot scale beyond one machine
â€¢ All eggs in one basket
```

## Message Service

### Purpose
Handles message creation, storage, and retrieval.

### Data Structures

```
Message:
    message_id: snowflake_id          // Globally unique, time-ordered
    conversation_id: string
    sender_id: string
    content: encrypted_bytes
    content_type: enum(text, image, video, ...)
    media_id: string (optional)
    reply_to: message_id (optional)
    sequence: int64                   // Per-conversation sequence number
    created_at: timestamp
    updated_at: timestamp
    status: enum(active, edited, deleted)
    
ConversationSequence:
    conversation_id: string
    next_sequence: atomic_int64
```

### Sequence Number Assignment

```
// Ensures message ordering within conversation
FUNCTION assign_sequence(conversation_id):
    // Atomic increment
    sequence = INCR conversation_sequences:{conversation_id}
    RETURN sequence

// Why this matters:
// - Messages might arrive at different servers
// - Network delays might reorder messages
// - Sequence provides total order within conversation
```

### Message Storage Strategy

```
WRITE PATH:
1. Validate message
2. Assign sequence number (atomic)
3. Write to primary message store (sync)
4. Replicate to secondary (async)
5. ACK to sender
6. Enqueue for delivery (async)

READ PATH:
1. Query by (conversation_id, sequence_range)
2. Cache recent messages per conversation
3. Return in sequence order

INDEXES:
â€¢ Primary: (conversation_id, sequence) â†’ message
â€¢ Secondary: (user_id, created_at) â†’ message (for inbox)
â€¢ Media: (media_id) â†’ message (for media access control)
```

### Failure Behavior

```
WRITE FAILURE:
â€¢ Retry with idempotency key (client-generated message_id)
â€¢ Duplicate writes detected and deduplicated
â€¢ Client shows "retry" if all retries fail

READ FAILURE:
â€¢ Serve from cache if available
â€¢ Return partial results with "more available" flag
â€¢ Client can retry failed ranges
```

## Delivery Service

### Purpose
Routes messages to recipients, handling online/offline cases.

### Data Structures

```
DeliveryTask:
    message_id: string
    conversation_id: string
    recipients: List<RecipientStatus>
    created_at: timestamp
    attempts: int
    next_retry_at: timestamp
    
RecipientStatus:
    user_id: string
    status: enum(pending, delivered, read, failed)
    delivered_at: timestamp (optional)
    device_deliveries: Map<device_id, DeliveryStatus>
```

### Delivery Algorithm

```
FUNCTION deliver_message(message):
    recipients = get_conversation_members(message.conversation_id)
    recipients.remove(message.sender_id)  // Don't deliver to sender
    
    FOR recipient IN recipients:
        // Check blocks
        IF is_blocked(recipient, message.sender_id):
            CONTINUE
        
        // Check online status
        IF is_online(recipient):
            TRY:
                push_via_websocket(recipient, message)
                mark_delivered(message.id, recipient)
            CATCH:
                enqueue_for_retry(message.id, recipient)
        ELSE:
            // Offline: send push notification
            send_push_notification(recipient, message)
            // Message will be fetched on next sync
            
    RETURN delivery_status
```

### Retry Strategy

```
RETRY POLICY:
â€¢ Immediate retry: 1 attempt
â€¢ After 1 second: 2nd attempt
â€¢ After 5 seconds: 3rd attempt
â€¢ After 30 seconds: 4th attempt
â€¢ After 5 minutes: 5th attempt
â€¢ After 1 hour: mark as "will deliver on sync"

RETRY QUEUE:
â€¢ Priority queue ordered by next_retry_at
â€¢ Separate queues per priority (high: 1:1, low: large groups)
â€¢ Dead letter queue for permanent failures
```

## Presence Service

### Purpose
Tracks user online/offline status with acceptable staleness.

### Design Philosophy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PRESENCE ANTI-PATTERNS (What NOT to do)                                   â”‚
â”‚                                                                             â”‚
â”‚   DON'T: Synchronous presence check on every message                        â”‚
â”‚   WHY: Adds latency to critical path                                        â”‚
â”‚                                                                             â”‚
â”‚   DON'T: Broadcast presence changes to all contacts                         â”‚
â”‚   WHY: O(NÂ²) problemâ€”1M users Ã— 100 contacts = 100M updates                 â”‚
â”‚                                                                             â”‚
â”‚   DON'T: Store presence in main database                                    â”‚
â”‚   WHY: Too high write volume, kills database                                â”‚
â”‚                                                                             â”‚
â”‚   DO: Lazy presence with subscription model                                 â”‚
â”‚   DO: Accept 30-60 second staleness                                         â”‚
â”‚   DO: Use in-memory store with TTL                                          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Structures

```
PresenceEntry:
    user_id: string
    status: enum(online, away, offline)
    last_seen: timestamp
    ttl: 120 seconds  // Auto-expire if not refreshed

PresenceSubscription:
    subscriber_id: string
    subscribed_to: Set<user_id>
    // Limited to ~100 subscriptions per user
```

### Presence Update Flow

```
USER COMES ONLINE:
1. WebSocket Gateway notifies Presence Service
2. Update: presence:{user_id} = {status: online, last_seen: now}
3. Set TTL: 120 seconds
4. Find subscribers watching this user
5. Push presence update to subscribers (async, best-effort)

HEARTBEAT REFRESH:
1. Every 60 seconds, client activity refreshes presence
2. Update last_seen, reset TTL
3. No broadcast (status unchanged)

USER GOES OFFLINE:
1. Explicit: Client sends "going offline" â†’ immediate update
2. Implicit: TTL expires (120 seconds no refresh)
3. Status becomes "offline" or "last seen X ago"
```

### Why This Design

```
PROBLEM: Naive presence is O(NÂ²)
â€¢ 100M online users
â€¢ Average 100 contacts each
â€¢ Presence change â†’ notify 100 people
â€¢ 100M users Ã— 100 contacts = 10B presence events

SOLUTION: Subscription with limits
â€¢ Only track presence for active conversations
â€¢ Limit subscriptions to ~100 users
â€¢ Only push to users with open app
â€¢ Reduces events by 1000x

ACCEPTABLE STALENESS:
â€¢ "Online" might be 30-60 seconds stale
â€¢ "Last seen 2 hours ago" is fine for messaging
â€¢ Typing indicators fill the real-time gap
```

---

# Part 7: Data Model & Storage Decisions

## Core Entities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA MODEL                                               â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  USERS                                                              â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ user_id (PK)                                                   â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ phone_number / email (unique, indexed)                         â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ display_name                                                   â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ avatar_url                                                     â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ created_at                                                     â”‚   â”‚
â”‚   â”‚  â””â”€â”€ settings (JSON: privacy, notifications, etc.)                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  CONVERSATIONS                                                      â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ conversation_id (PK)                                           â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ type (1:1 | group)                                             â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ name (for groups)                                              â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ created_at                                                     â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ created_by                                                     â”‚   â”‚
â”‚   â”‚  â””â”€â”€ metadata (JSON: settings, last_activity, etc.)                 â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  CONVERSATION_MEMBERS                                               â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ conversation_id (PK)                                           â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ user_id (PK)                                                   â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ role (member | admin)                                          â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ joined_at                                                      â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ last_read_sequence (for unread count)                          â”‚   â”‚
â”‚   â”‚  â””â”€â”€ muted_until                                                    â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  MESSAGES                                                           â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ message_id (PK, snowflake)                                     â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ conversation_id (indexed)                                      â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ sequence (per-conversation order)                              â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ sender_id                                                      â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ content (encrypted)                                            â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ content_type                                                   â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ media_id (FK to media)                                         â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ reply_to (FK to message)                                       â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ created_at                                                     â”‚   â”‚
â”‚   â”‚  â””â”€â”€ status (active | edited | deleted)                             â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  USER_INBOX (Denormalized for sync performance)                     â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ user_id (partition key)                                        â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ message_id (sort key)                                          â”‚   â”‚
â”‚   â”‚  â”œâ”€â”€ conversation_id                                                â”‚   â”‚
â”‚   â”‚  â””â”€â”€ created_at                                                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Storage Technology Choices

```
MESSAGE STORE: Distributed Wide-Column Store (Cassandra/Bigtable)
WHY:
â€¢ Handles massive write throughput (10M/sec)
â€¢ Excellent for time-series data (messages ordered by time)
â€¢ Good partition strategy: conversation_id
â€¢ Easy horizontal scaling
â€¢ Tunable consistency (per-query)

USER/CONVERSATION METADATA: Relational Database (Sharded MySQL/PostgreSQL)
WHY:
â€¢ Complex queries (find conversation by members)
â€¢ Strong consistency needed for membership
â€¢ Relatively low write volume
â€¢ ACID transactions for group operations

PRESENCE: In-Memory Store (Redis Cluster)
WHY:
â€¢ Extremely fast reads/writes
â€¢ Built-in TTL for auto-expiry
â€¢ Pub/sub for presence updates
â€¢ Ephemeral data (can lose and rebuild)

MEDIA: Object Storage (S3/GCS)
WHY:
â€¢ Unlimited scale
â€¢ Cost-effective for large files
â€¢ Built-in CDN integration
â€¢ Lifecycle policies for archival

CACHE: Distributed Cache (Redis/Memcached)
WHY:
â€¢ Hot conversation metadata
â€¢ Recent message cache
â€¢ User profile cache
â€¢ Session/auth token cache
```

## Partitioning Strategy

```
MESSAGES:
Partition key: conversation_id
Sort key: sequence

Why:
â€¢ All messages in conversation are co-located
â€¢ Reads are always within single partition
â€¢ Writes distributed across conversations
â€¢ Hot conversations don't affect others

POTENTIAL ISSUE: Hot conversation (celebrity with millions)
Solution: Sub-partition large conversations by time bucket

USER_INBOX:
Partition key: user_id
Sort key: message_id

Why:
â€¢ Sync query is always for single user
â€¢ Message_id is time-ordered (snowflake)
â€¢ Efficient range queries for "messages since X"
```

## Retention Policies

```
MESSAGE CONTENT:
â€¢ Hot storage (fast SSD): 30 days
â€¢ Warm storage (HDD): 1 year
â€¢ Cold storage (archive): 7 years (legal/compliance)
â€¢ User-deleted: Remove from all tiers

MEDIA:
â€¢ Original: 30 days
â€¢ Thumbnails: 1 year
â€¢ After expiry: Show "media expired" placeholder
â€¢ User can re-upload if they have local copy

METADATA:
â€¢ Conversation metadata: Forever (small)
â€¢ Message metadata: Same as content
â€¢ Delivery receipts: 30 days
â€¢ Read receipts: 7 days

PRESENCE/TYPING:
â€¢ No persistence (in-memory only)
â€¢ Reconstruct on service restart
```

## Schema Evolution

```
CHALLENGE: Billions of messages, can't do ALTER TABLE

STRATEGY 1: New columns are nullable with defaults
â€¢ Add new column with default value
â€¢ Old messages have default
â€¢ New messages have explicit value
â€¢ Backfill async if needed

STRATEGY 2: Schema versioning
â€¢ Each message has schema_version field
â€¢ Reader handles multiple versions
â€¢ Writer always uses latest version
â€¢ Gradual migration over time

STRATEGY 3: Flexible schema
â€¢ Content stored as JSON/protobuf
â€¢ Schema defined by content_type
â€¢ New types don't require DB changes

EXAMPLE: Adding reactions
V1: No reactions
V2: reactions: Map<emoji, count>
V3: reactions: List<{emoji, user_id, timestamp}>

Migration:
â€¢ V1 messages: reactions = null (no reactions existed)
â€¢ V2 messages: Migrate to V3 format on read
â€¢ V3 messages: Full data
```

## Why Other Data Models Were Rejected

```
REJECTED: Single MySQL for everything
Problems:
â€¢ Cannot handle 10M writes/sec
â€¢ Joins become bottleneck
â€¢ Sharding MySQL is complex
â€¢ No built-in time-series optimization

REJECTED: MongoDB for messages
Problems:
â€¢ Document model doesn't fit conversation pattern
â€¢ Ordering guarantees weaker
â€¢ Operational complexity at scale
â€¢ Better options exist for this workload

REJECTED: Storing messages in S3 directly
Problems:
â€¢ No efficient range queries
â€¢ No real-time writes
â€¢ High latency for recent messages
â€¢ Works for archive, not hot path
```

---

# Part 8: Consistency, Concurrency & Ordering

## The Core Ordering Challenge

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MESSAGE ORDERING: THE HARD PROBLEM                       â”‚
â”‚                                                                             â”‚
â”‚   SCENARIO: Alice and Bob messaging simultaneously                          â”‚
â”‚                                                                             â”‚
â”‚   Alice's phone           Network              Bob's phone                  â”‚
â”‚   10:00:00.001            â”€â”€â”€â”€â”€â”€â”€â”€â–º            10:00:00.100                 â”‚
â”‚   "Hello"                                      "Hi there"                   â”‚
â”‚                           â—„â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚
â”‚   10:00:00.150                                 10:00:00.050                 â”‚
â”‚   "How are you?"                               "What's up?"                 â”‚
â”‚                                                                             â”‚
â”‚   PROBLEM: Whose "Hello" comes first?                                       â”‚
â”‚   â€¢ Alice's clock: Alice first (10:00:00.001 vs 10:00:00.100)               â”‚
â”‚   â€¢ Bob's clock: Bob first (10:00:00.050 vs 10:00:00.150)                   â”‚
â”‚   â€¢ Server receipt: Depends on network latency                              â”‚
â”‚                                                                             â”‚
â”‚   THERE IS NO "CORRECT" ANSWER                                              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Ordering Guarantees We Provide

```
GUARANTEE 1: Per-sender ordering (STRICT)
â€¢ Messages from same sender appear in send order
â€¢ Implemented via per-sender sequence numbers
â€¢ Client cannot send message N+1 until N is ACKed

GUARANTEE 2: Per-conversation causal ordering (STRONG)
â€¢ If Alice reads Bob's message then replies, reply comes after
â€¢ Implemented via Lamport-style logical clocks
â€¢ Each message carries "I have seen message X" metadata

GUARANTEE 3: Cross-sender ordering (BEST EFFORT)
â€¢ Use server-assigned timestamps as tiebreaker
â€¢ Within same millisecond: arbitrary but consistent
â€¢ Users rarely notice cross-sender ordering issues

NO GUARANTEE: Cross-conversation ordering
â€¢ Message to Group A and message to Friend B have no ordering
â€¢ Different conversations are independent
â€¢ This is fineâ€”users don't compare across conversations
```

## Implementation: Logical Clocks

```
LAMPORT TIMESTAMP PER CONVERSATION:

Message:
    message_id: string
    logical_time: int64
    sender_sequence: int64
    causal_dependencies: List<message_id>

SEND MESSAGE:
1. Client tracks: max_seen_logical_time
2. new_logical_time = max_seen_logical_time + 1
3. Include: causal_dependencies = [last_message_I_saw]
4. Server validates: new_time > all dependency times
5. If conflict: Server assigns higher time

RECEIVE MESSAGE:
1. Update: max_seen_logical_time = max(current, received)
2. Sort messages by logical_time
3. If gap in sequence: Wait or fetch missing

CONFLICT RESOLUTION:
Same logical time? Use (logical_time, sender_id) as total order
Sender_id comparison is arbitrary but consistent
```

## Race Conditions

### Race 1: Simultaneous Send to Same Conversation

```
Alice and Bob both send at T=0

Server A receives Alice's message
Server B receives Bob's message

WITHOUT COORDINATION:
â€¢ Both assigned sequence=1
â€¢ Conflict when syncing

SOLUTION: Centralized sequence assignment
â€¢ Conversation sequence stored in Redis
â€¢ INCR is atomic
â€¢ Both servers get unique sequence
â€¢ May have gaps if one fails, but ordering preserved
```

### Race 2: Edit During Delivery

```
Alice sends "Hello"
Message in transit to Bob
Alice edits to "Hello!"
Edit arrives before original

Bob's view:
1. Receives edit for message he doesn't have
2. What to do?

SOLUTION: Idempotent operations with version
â€¢ Each message has version number
â€¢ Edit: version++ with full content
â€¢ Client applies edits to latest known version
â€¢ If base message arrives after edit: Apply edit
```

### Race 3: Group Membership Change During Send

```
Alice sends to group of 10 members
During fanout, Carol is removed from group

Question: Should Carol receive the message?

SOLUTION: Snapshot membership at send time
â€¢ When message stored, snapshot member list
â€¢ Fanout uses snapshot, not current membership
â€¢ Carol receives (was member at send time)
â€¢ Future messages: Carol not included

Alternative: Use current membership
â€¢ Carol doesn't receive
â€¢ But this creates strange gaps in Carol's history
â€¢ Staff choice: Snapshot (more intuitive for users)
```

## Idempotency

```
CLIENT-GENERATED MESSAGE ID:

PROBLEM: 
Client sends message, network fails before ACK
Client retriesâ€”is this a duplicate or new message?

SOLUTION:
Client generates unique message_id before first attempt
Server uses message_id for deduplication

FUNCTION receive_message(message):
    existing = get_message_by_id(message.id)
    
    IF existing:
        // Duplicate - return existing
        RETURN existing
    ELSE:
        // New message - store and process
        store(message)
        RETURN message

IDEMPOTENCY WINDOW:
â€¢ Keep message_id lookup for 24 hours
â€¢ After 24 hours: Unlikely duplicate (user would notice)
â€¢ Very late retry: Accept as new (rare edge case)
```

## Clock Assumptions

```
WHAT WE ASSUME:
â€¢ Server clocks are synchronized via NTP
â€¢ Server clock skew < 100ms (acceptable)
â€¢ Client clocks can be arbitrarily wrong
â€¢ Network latency is unbounded

IMPLICATIONS:
â€¢ Never trust client timestamps for ordering
â€¢ Client timestamp for display: "Sent at 10:42am"
â€¢ Server timestamp for ordering and sync
â€¢ Logical clocks for causal ordering

WHEN CLOCKS GO WRONG:
Client 1 year ahead:
â€¢ Server rejects timestamp, uses server time
â€¢ Message sorts correctly
â€¢ Display time might be wrong briefly

Server 1 hour behind:
â€¢ Messages from this server have old timestamps
â€¢ Sort among themselves correctly
â€¢ May sort before messages from other servers
â€¢ Self-heals when NTP corrects

STAFF INSIGHT:
Clock issues are inevitable at scale. Design
so that clock problems cause cosmetic issues
(wrong display time) not correctness issues
(wrong message order).
```

---

# Part 9: Failure Modes & Degradation

## Failure Categories

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FAILURE SEVERITY LEVELS                                  â”‚
â”‚                                                                             â”‚
â”‚   LEVEL 1: INVISIBLE TO USERS                                               â”‚
â”‚   â€¢ Single server failure with quick failover                               â”‚
â”‚   â€¢ Cache miss (falls back to database)                                     â”‚
â”‚   â€¢ Replica lag (reads slightly stale data)                                 â”‚
â”‚                                                                             â”‚
â”‚   LEVEL 2: COSMETIC DEGRADATION                                             â”‚
â”‚   â€¢ Typing indicators stop working                                          â”‚
â”‚   â€¢ Presence shows stale data                                               â”‚
â”‚   â€¢ Read receipts delayed                                                   â”‚
â”‚   â€¢ Push notifications delayed                                              â”‚
â”‚                                                                             â”‚
â”‚   LEVEL 3: NOTICEABLE DEGRADATION                                           â”‚
â”‚   â€¢ Message delivery delayed (seconds to minutes)                           â”‚
â”‚   â€¢ Slow conversation loading                                               â”‚
â”‚   â€¢ Media upload/download failing                                           â”‚
â”‚                                                                             â”‚
â”‚   LEVEL 4: SIGNIFICANT IMPACT                                               â”‚
â”‚   â€¢ Messages not delivering (but stored)                                    â”‚
â”‚   â€¢ Cannot send new messages                                                â”‚
â”‚   â€¢ Cannot load conversations                                               â”‚
â”‚                                                                             â”‚
â”‚   LEVEL 5: CATASTROPHIC                                                     â”‚
â”‚   â€¢ Message loss (data corruption)                                          â”‚
â”‚   â€¢ Messages delivered to wrong users                                       â”‚
â”‚   â€¢ Extended complete outage                                                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Partial Failures

### WebSocket Gateway Failure

```
FAILURE: 10% of WebSocket gateways crash

IMPACT:
â€¢ 10% of users disconnected
â€¢ Messages to those users queue up
â€¢ Reconnections storm to remaining servers

MITIGATION:
â€¢ Clients auto-reconnect with exponential backoff
â€¢ Load balancer removes dead servers
â€¢ Remaining servers handle increased connections
â€¢ Message queue drains when users reconnect

DEGRADATION:
â€¢ 10% of users: 10-30 second delay
â€¢ 90% of users: Unaffected
â€¢ No message loss

RECOVERY TIME: 30 seconds to 2 minutes
```

### Message Store Failure

```
FAILURE: Primary message store unavailable

IMPACT:
â€¢ Cannot write new messages
â€¢ Cannot read recent messages (not in cache)

MITIGATION:
â€¢ Switch to secondary replica (async, may be behind)
â€¢ Queue writes in memory/local disk
â€¢ Return "sending..." to client, complete async
â€¢ Serve cached conversations for reads

DEGRADATION:
â€¢ Writes: Delayed, client shows "sending..."
â€¢ Reads: Recent messages might be missing
â€¢ No data loss if primary recovers

RECOVERY TIME: Depends on primary repair
```

### Presence Service Failure

```
FAILURE: Presence service completely down

IMPACT:
â€¢ Online status not updating
â€¢ "Last seen" stuck at failure time
â€¢ Delivery routing guesses online/offline

MITIGATION:
â€¢ Cache last-known presence (stale)
â€¢ Assume everyone potentially online
â€¢ Send both push and WebSocket (wasteful but works)

DEGRADATION:
â€¢ Presence shows stale data
â€¢ Some unnecessary push notifications
â€¢ Core messaging unaffected

RECOVERY TIME: Immediate once service restarts
```

## Slow Dependencies

```
MESSAGE STORE SLOW (P99 > 1 second):

Detection:
â€¢ Latency metrics alert
â€¢ Queue depth growing
â€¢ Timeout rate increasing

Response:
â€¢ Shed non-critical operations (analytics events)
â€¢ Increase timeout for critical writes
â€¢ Serve reads from cache more aggressively
â€¢ Consider circuit breaker if persistent

PUSH NOTIFICATION SERVICE SLOW:

Detection:
â€¢ Push delivery latency increasing
â€¢ Push queue growing

Response:
â€¢ Push is non-blocking anyway
â€¢ Increase queue capacity
â€¢ Drop low-priority pushes (read receipts)
â€¢ Batch more aggressively
```

## Retry Storms

```
SCENARIO: Message store recovers after 5-minute outage

PROBLEM:
â€¢ 5 minutes of messages queued
â€¢ All messages try to deliver at once
â€¢ Store overwhelmed again

PREVENTION:
â€¢ Jitter in retry timing
â€¢ Gradual ramp-up after recovery
â€¢ Priority queue (1:1 before groups, recent before old)
â€¢ Rate limit writes per second

IMPLEMENTATION:
FUNCTION calculate_retry_delay(attempt, base_delay):
    delay = base_delay * (2 ^ attempt)
    jitter = random(0, delay * 0.1)
    RETURN delay + jitter

// Retry at 1s, 2s, 4s, 8s... with 10% jitter
// Spreads retries over time window
```

## Data Corruption

```
SCENARIO: Bug causes messages stored with wrong conversation_id

DETECTION:
â€¢ Users report seeing wrong messages
â€¢ Message count mismatch in conversations
â€¢ Anomaly detection on message patterns

IMMEDIATE RESPONSE:
1. Identify scope (how many messages affected)
2. Stop the bleeding (fix or rollback bad code)
3. Quarantine affected data
4. Communicate to affected users

RECOVERY:
â€¢ If recoverable: Repair conversation_id from metadata
â€¢ If not: Mark messages as "recovered" with caveats
â€¢ Worst case: Inform users of data loss

PREVENTION:
â€¢ Strong typing on conversation_id
â€¢ Foreign key constraints (where possible)
â€¢ Write-time validation
â€¢ Audit log for forensics
```

## Control-Plane Failures

```
CONFIG SERVICE DOWN:

Impact:
â€¢ Cannot update rate limits
â€¢ Cannot change feature flags
â€¢ Cannot modify routing rules

Mitigation:
â€¢ Servers cache last-known config
â€¢ Stale config is usually fine
â€¢ Critical configs have long TTL

GROUP MANAGEMENT SERVICE DOWN:

Impact:
â€¢ Cannot create new groups
â€¢ Cannot add/remove members
â€¢ Existing groups work fine

Mitigation:
â€¢ Queue group operations
â€¢ Notify users of delay
â€¢ Retry on recovery
```

## Failure Timeline Walkthrough

```
T+0:00 - Primary message database becomes unresponsive
â”‚
T+0:05 - Health checks fail, alerts fire
â”‚
T+0:10 - Automatic failover to secondary begins
â”‚        â€¢ Writes queue in memory
â”‚        â€¢ Reads served from cache
â”‚        â€¢ Clients see "sending..." stuck
â”‚
T+0:30 - Failover complete, secondary promoted
â”‚        â€¢ New writes go to new primary
â”‚        â€¢ Queued writes drain (FIFO)
â”‚        â€¢ Some writes during transition may retry
â”‚
T+1:00 - Queue drained, normal operation
â”‚        â€¢ Some messages delayed by 30-60 seconds
â”‚        â€¢ Order preserved within conversations
â”‚
T+5:00 - Old primary recovered, becomes secondary
â”‚        â€¢ Catches up via replication
â”‚        â€¢ No user impact
â”‚
POST-MORTEM:
â€¢ 60 seconds of elevated latency
â€¢ 0 messages lost
â€¢ 0.1% of users experienced noticeable delay
â€¢ Root cause: Disk I/O saturation from runaway query
```

## Graceful Degradation Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DEGRADATION PRIORITY                                     â”‚
â”‚                                                                             â”‚
â”‚   PROTECT AT ALL COSTS:                                                     â”‚
â”‚   1. Message storage (never lose a message)                                 â”‚
â”‚   2. Message delivery (delay OK, lose not OK)                               â”‚
â”‚   3. Message ordering (within conversation)                                 â”‚
â”‚                                                                             â”‚
â”‚   DEGRADE FIRST (SHED LOAD):                                                â”‚
â”‚   1. Typing indicators (cosmetic)                                           â”‚
â”‚   2. Read receipts (cosmetic)                                               â”‚
â”‚   3. Presence updates (stale is fine)                                       â”‚
â”‚   4. Media thumbnails (show placeholder)                                    â”‚
â”‚   5. Search (can fail gracefully)                                           â”‚
â”‚                                                                             â”‚
â”‚   DEGRADATION TOGGLES:                                                      â”‚
â”‚   â€¢ disable_typing_indicators: true                                         â”‚
â”‚   â€¢ disable_read_receipts: true                                             â”‚
â”‚   â€¢ presence_staleness_ok: 300 (seconds)                                    â”‚
â”‚   â€¢ disable_media_preview: true                                             â”‚
â”‚   â€¢ message_delivery_best_effort: true                                      â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 10: Performance Optimization & Hot Paths

## Critical Paths

```
PATH 1: SEND MESSAGE (Most critical)
User action â†’ ACK visible

Target: < 100ms P50, < 300ms P95

Steps:
1. TLS termination: 5ms
2. Auth validation: 5ms (cached token)
3. Rate limit check: 1ms (local)
4. Message validation: 1ms
5. Sequence assignment: 5ms (Redis)
6. Message write: 30ms (database)
7. Response: 5ms
Total: ~52ms typical

OPTIMIZATIONS APPLIED:
â€¢ Auth token cached in gateway
â€¢ Rate limit state in local memory
â€¢ Async delivery (not in critical path)
â€¢ Connection reuse (no handshake per message)

PATH 2: RECEIVE MESSAGE (Second most critical)
Message stored â†’ User sees it

Target: < 500ms P50

Steps:
1. Delivery service picks up: 10ms
2. Find user connections: 5ms
3. Route to WebSocket server: 10ms
4. Push to client: 20ms
5. Client renders: 50ms (client-side)
Total: ~95ms (excluding client)

PATH 3: LOAD CONVERSATION (User experience critical)
User opens chat â†’ Messages visible

Target: < 200ms P50

Steps:
1. Auth + conversation access check: 10ms
2. Query recent messages: 30ms (cached)
3. Fetch member info: 10ms (cached)
4. Response serialization: 5ms
5. Client render: 100ms (client-side)
Total: ~55ms server-side
```

## Caching Strategies

```
CACHE LAYER 1: Client-side
â€¢ Full message history for recent conversations
â€¢ User profiles for contacts
â€¢ Conversation list with snippets
â€¢ Media thumbnails

Benefits: 0ms latency for cached data
Invalidation: Server push or pull on open

CACHE LAYER 2: API Gateway (CDN edge)
â€¢ Static assets (avatars, stickers)
â€¢ Public media
â€¢ Not used for messages (personalized)

CACHE LAYER 3: Service Cache (Redis)
Cached:
â€¢ Conversation metadata: 60 second TTL
â€¢ User profiles: 300 second TTL
â€¢ Recent messages per conversation: 60 second TTL
â€¢ Group membership: 60 second TTL

Not cached:
â€¢ Unread counts (changes too frequently)
â€¢ Presence (separate system)
â€¢ Auth tokens (security concern)

CACHE LAYER 4: Database Query Cache
â€¢ Common query patterns cached at DB level
â€¢ Automatic invalidation on write
```

## Cache Invalidation

```
CONVERSATION CACHE INVALIDATION:

Trigger events:
â€¢ New message â†’ Invalidate "recent messages" cache
â€¢ Member change â†’ Invalidate "membership" cache
â€¢ Settings change â†’ Invalidate "metadata" cache

Strategy: Write-through with async invalidation
1. Write to database (synchronous)
2. Update cache (synchronous)
3. Broadcast invalidation to other cache nodes (async)

RACE CONDITION:
Read from stale cache while write in progress

Mitigation:
â€¢ Use cache-aside with short TTL
â€¢ Accept eventual consistency
â€¢ Critical reads bypass cache
```

## Precomputation vs Runtime Work

```
PRECOMPUTED:
â€¢ Unread count per conversation
  - Updated on every message write/read
  - Query: O(1) lookup
  - Without: O(messages) count at runtime

â€¢ Conversation list order (by last_message_time)
  - Updated on every message
  - Query: Sorted list, O(1)
  - Without: O(conversations) sort at runtime

â€¢ Group member list (denormalized)
  - Updated on membership change
  - Query: O(1) lookup
  - Without: O(members) join at runtime

COMPUTED AT RUNTIME:
â€¢ Search results (too variable to precompute)
â€¢ Media transcoding (done on upload, cached)
â€¢ Message formatting (done on client)

HYBRID:
â€¢ Read receipts aggregation
  - Individual receipts stored
  - Aggregated "3 people read" computed on query
  - Cached for 30 seconds
```

## Backpressure

```
WEBSOCKET GATEWAY BACKPRESSURE:

Monitor:
â€¢ Pending message queue per connection
â€¢ If queue > 1000 messages: Connection is slow

Response:
1. Stop queuing new messages for this connection
2. Drop cosmetic messages (typing, presence)
3. Keep message delivery in separate priority queue
4. If sustained: Close connection, force resync

MESSAGE STORE BACKPRESSURE:

Monitor:
â€¢ Write latency > threshold
â€¢ Queue depth growing

Response:
1. Start rejecting non-critical writes
2. Return "server busy" to clients (429)
3. Clients back off with exponential retry
4. Shed read traffic to replicas
```

## Load Shedding

```
PRIORITY LEVELS:
P0: Message send ACK (never shed)
P1: Message delivery (shed only in emergency)
P2: Message history reads (shed before delivery)
P3: Typing indicators (shed early)
P4: Analytics events (always shed first)

SHEDDING IMPLEMENTATION:

FUNCTION handle_request(request):
    priority = get_priority(request.type)
    load = get_current_load()
    
    IF load > CRITICAL_THRESHOLD:
        IF priority > P1:
            RETURN 503("Service Overloaded")
    
    IF load > HIGH_THRESHOLD:
        IF priority > P2:
            RETURN 503("Service Overloaded")
    
    // Process normally
    process(request)

HEADERS:
Retry-After: 5  // Tell client when to retry
X-Shed-Reason: load-shedding  // For debugging
```

## Optimizations NOT Done (and Why)

```
NOT DONE: Compress all messages
WHY: Most messages are tiny (< 100 bytes)
     Compression overhead > savings
     Only compress media

NOT DONE: Global deduplication of media
WHY: Privacy concerns (hash reveals same photo)
     Computation overhead
     Storage is cheap

NOT DONE: Predictive message prefetch
WHY: Hard to predict which conversation opens next
     Wastes bandwidth for wrong predictions
     Client cache sufficient

NOT DONE: Inline media in message payload
WHY: Delays message delivery for slow media
     Better UX: Show text immediately, load media async

NOT DONE: Real-time analytics
WHY: Hot path shouldn't feed analytics
     Sample or async pipeline instead
     Analytics can be delayed
```

---

# Part 11: Cost & Efficiency

## Major Cost Drivers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COST BREAKDOWN (at WhatsApp scale)                       â”‚
â”‚                                                                             â”‚
â”‚   1. MEDIA STORAGE (40% of cost)                                            â”‚
â”‚      â€¢ 30 PB/day new media                                                  â”‚
â”‚      â€¢ Hot storage: $0.023/GB/month                                         â”‚
â”‚      â€¢ 30 days hot: 900 PB Ã— $0.023 = $20M/month                            â”‚
â”‚                                                                             â”‚
â”‚   2. BANDWIDTH (25% of cost)                                                â”‚
â”‚      â€¢ Media download: 100 PB/day outbound                                  â”‚
â”‚      â€¢ At $0.05/GB: $5M/day = $150M/month                                   â”‚
â”‚      â€¢ With CDN: ~$50M/month                                                â”‚
â”‚                                                                             â”‚
â”‚   3. COMPUTE (20% of cost)                                                  â”‚
â”‚      â€¢ WebSocket servers: 10,000 servers                                    â”‚
â”‚      â€¢ Message services: 5,000 servers                                      â”‚
â”‚      â€¢ Supporting services: 5,000 servers                                   â”‚
â”‚      â€¢ Total: ~20,000 servers                                               â”‚
â”‚                                                                             â”‚
â”‚   4. DATABASE (10% of cost)                                                 â”‚
â”‚      â€¢ Message store: Petabytes SSD                                         â”‚
â”‚      â€¢ User/conversation metadata: Terabytes                                â”‚
â”‚      â€¢ Managed database services                                            â”‚
â”‚                                                                             â”‚
â”‚   5. THIRD-PARTY SERVICES (5% of cost)                                      â”‚
â”‚      â€¢ Push notification (APNs/FCM)                                         â”‚
â”‚      â€¢ CDN                                                                  â”‚
â”‚      â€¢ SMS verification                                                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## How Cost Scales with Traffic

```
LINEAR SCALING:
â€¢ Message storage: Each message costs storage
â€¢ Bandwidth: Each media download costs bandwidth
â€¢ Push notifications: Each notification has marginal cost

SUB-LINEAR SCALING:
â€¢ Fixed infrastructure: Doesn't scale with messages
â€¢ Caching: More traffic = better cache hit rate
â€¢ Batch operations: Larger batches = lower per-item cost

SUPER-LINEAR SCALING (Watch out!):
â€¢ Group messaging: 1000-member group = 1000x fanout
â€¢ Presence: N users Ã— M contacts = NÃ—M updates
â€¢ Search: Larger index = slower queries
```

## Cost vs Reliability Trade-offs

```
TRADE-OFF 1: Replication Factor
â€¢ 3x replication: High durability, 3x storage cost
â€¢ 2x replication: Acceptable durability, 2x cost
â€¢ 1x replication: Risky, lowest cost
DECISION: 3x for messages (critical), 2x for media (can re-upload)

TRADE-OFF 2: Hot vs Cold Storage
â€¢ All hot: Fast access, expensive
â€¢ Tiered: Fast for recent, slow for old
â€¢ All cold: Cheap but slow
DECISION: Tier at 30 days (95% of reads are < 30 days)

TRADE-OFF 3: Push Notification Aggressiveness
â€¢ Push every message: Best UX, highest cost
â€¢ Batch pushes: Good UX, lower cost
â€¢ Push only for direct mentions: Okay UX, lowest cost
DECISION: Immediate for 1:1, batched for active groups

TRADE-OFF 4: Media Quality
â€¢ Original quality: Best UX, huge storage
â€¢ Aggressive compression: Lower quality, much smaller
â€¢ Resolution tiers: Balance
DECISION: Compress to "good enough" quality, offer original download
```

## What Over-Engineering Looks Like

```
OVER-ENGINEERING EXAMPLE 1:
"We need exactly-once delivery with global consensus"

Reality:
â€¢ At-least-once with client dedup is sufficient
â€¢ Global consensus adds 100ms+ latency
â€¢ Duplicates are rare and harmless
â€¢ Cost: 10x infrastructure for 0.01% edge case

OVER-ENGINEERING EXAMPLE 2:
"We need sub-10ms message delivery globally"

Reality:
â€¢ Physics limits: 100ms cross-continent
â€¢ Users don't perceive < 200ms differences
â€¢ Optimizing below 100ms requires edge compute
â€¢ Cost: 5x infrastructure for imperceptible improvement

OVER-ENGINEERING EXAMPLE 3:
"We need to store all messages forever in hot storage"

Reality:
â€¢ 99% of reads are last 7 days
â€¢ Users rarely access year-old messages
â€¢ Hot storage is 10x cost of cold
â€¢ Cost: 10x storage for 0.1% of reads
```

## Cost-Aware Redesign

```
REDESIGN: Media Storage Optimization

BEFORE:
â€¢ Store original + 3 thumbnail sizes
â€¢ All in hot storage
â€¢ 4x storage per image
â€¢ Cost: $80M/month

AFTER:
â€¢ Store original in hot for 7 days
â€¢ Generate thumbnails on-demand (cached)
â€¢ Move to cold after 7 days
â€¢ Regenerate thumbnails from cold if needed
â€¢ Cost: $30M/month

TRADE-OFF:
â€¢ Cold media access adds 500ms latency
â€¢ Users accept this for old photos
â€¢ Savings: $50M/month

REDESIGN: Presence Efficiency

BEFORE:
â€¢ Update presence every 10 seconds
â€¢ Broadcast to all contacts
â€¢ 100M users Ã— 100 contacts Ã— 6/min = 60B updates/min

AFTER:
â€¢ Update presence every 60 seconds
â€¢ Only broadcast to users with app open
â€¢ Lazy presence (query on conversation open)
â€¢ Updates: 500M/min (100x reduction)

TRADE-OFF:
â€¢ Presence up to 60 seconds stale
â€¢ Users don't notice
â€¢ Compute savings: 90%
```

---

# Part 12: Multi-Region & Global Considerations

## Data Locality

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA LOCALITY STRATEGY                                   â”‚
â”‚                                                                             â”‚
â”‚   PRINCIPLE: Keep conversation data close to participants                  â”‚
â”‚                                                                             â”‚
â”‚   USER DATA:                                                                â”‚
â”‚   â€¢ Stored in user's home region                                            â”‚
â”‚   â€¢ Determined by phone number prefix                                       â”‚
â”‚   â€¢ Can migrate if user moves (rare)                                        â”‚
â”‚                                                                             â”‚
â”‚   CONVERSATION DATA:                                                        â”‚
â”‚   â€¢ 1:1: Stored in region of user with most activity                       â”‚
â”‚   â€¢ Group: Stored in region with plurality of members                       â”‚
â”‚   â€¢ Cross-region conversation: Replicate to both regions                   â”‚
â”‚                                                                             â”‚
â”‚   EXAMPLE:                                                                  â”‚
â”‚   Alice (US) and Bob (US): Data in US                                       â”‚
â”‚   Alice (US) and Chen (Asia): Replicate US â†” Asia                          â”‚
â”‚   Group with 8 US, 2 EU: Data in US, async replicate to EU                 â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Replication Strategies

```
MESSAGE REPLICATION:

SYNCHRONOUS (within region):
â€¢ Primary write + 2 replicas in same region
â€¢ Ensures durability before ACK
â€¢ Latency: +5-10ms

ASYNCHRONOUS (cross-region):
â€¢ After ACK, replicate to other regions
â€¢ Disaster recovery (not active use)
â€¢ Lag: 100ms - 5 seconds typical

USER DATA REPLICATION:
â€¢ Master in home region
â€¢ Read replicas in other regions
â€¢ Writes always go to home region

CONVERSATION METADATA:
â€¢ Sync replicated within region (strong consistency)
â€¢ Async replicated cross-region (eventual consistency)
```

## Traffic Routing

```
ROUTING LOGIC:

USER CONNECTS:
1. DNS returns nearest edge location
2. Edge handles WebSocket connection
3. Requests routed to user's home region

MESSAGE SEND:
1. Accept at any region (where user is connected)
2. Forward to conversation's home region
3. Store and assign sequence there
4. Fan out delivery from there

CROSS-REGION MESSAGE:
Alice (US) sends to Bob (EU):
1. Alice's client â†’ US edge
2. US edge â†’ US message service (store)
3. US â†’ EU (async replication for Bob's home region)
4. Delivery service â†’ Bob via EU edge

OPTIMIZATION:
If Bob is online at EU edge:
â€¢ Push message directly via EU
â€¢ Don't wait for replication
â€¢ Mark as "delivered but may not be in EU replica yet"
```

## Failure Across Regions

```
SINGLE REGION FAILURE:

Scenario: US-West completely down

Impact:
â€¢ Users whose home is US-West: Degraded service
â€¢ Users in other regions: Minor impact (some contacts unavailable)

Response:
1. DNS removes US-West from rotation
2. US-West users routed to US-East (higher latency)
3. US-East becomes primary for US-West data
4. Catch up from replicas
5. When US-West recovers: Sync back

Message handling:
â€¢ Messages to US-West users queue in other regions
â€¢ Messages from US-West users sent via reroute
â€¢ Some latency, no message loss

NETWORK PARTITION:

Scenario: US-West and US-East cannot communicate

Impact:
â€¢ Split-brain potential
â€¢ Users in each region only see users in their region

Response:
1. Each region operates independently
2. Cross-region messages queue
3. When partition heals: Merge messages
4. Conflict resolution: Later timestamp wins

Conflict example:
â€¢ During partition, Alice (US-West) and Bob (US-East) both delete same group
â€¢ Partition heals
â€¢ Resolution: Keep one delete, apply to both
```

## When Multi-Region Is NOT Worth It

```
SKIP MULTI-REGION IF:
â€¢ All users in one geography
â€¢ < 1M users (cost doesn't justify)
â€¢ Can tolerate 30-minute failover RTO
â€¢ No regulatory data residency requirements

MULTI-REGION COSTS:
â€¢ 2-3x infrastructure (replicate everything)
â€¢ 2-3x operational complexity
â€¢ Cross-region bandwidth charges
â€¢ More complex debugging

SIMPLER ALTERNATIVE:
â€¢ Single region with good backups
â€¢ Automated restore to new region
â€¢ RTO: 30 minutes to 4 hours
â€¢ Cost: Much lower

WHEN IT IS WORTH IT:
â€¢ Global user base
â€¢ < 1 minute RTO requirement
â€¢ Data residency requirements (EU data in EU)
â€¢ > 10M users
```

---

# Part 13: Security & Abuse Considerations

## Abuse Vectors

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ABUSE VECTORS AND MITIGATIONS                            â”‚
â”‚                                                                             â”‚
â”‚   SPAM:                                                                     â”‚
â”‚   â€¢ Mass messaging to strangers                                             â”‚
â”‚   â€¢ Group join spam                                                         â”‚
â”‚   â€¢ Media/link spam                                                         â”‚
â”‚   Mitigation:                                                               â”‚
â”‚   â€¢ Rate limit messages to non-contacts                                     â”‚
â”‚   â€¢ Rate limit group joins                                                  â”‚
â”‚   â€¢ Link/media scanning                                                     â”‚
â”‚   â€¢ ML-based spam detection                                                 â”‚
â”‚                                                                             â”‚
â”‚   HARASSMENT:                                                               â”‚
â”‚   â€¢ Repeated unwanted messages                                              â”‚
â”‚   â€¢ Account creation to bypass blocks                                       â”‚
â”‚   â€¢ Group invite harassment                                                 â”‚
â”‚   Mitigation:                                                               â”‚
â”‚   â€¢ Easy blocking                                                           â”‚
â”‚   â€¢ Phone number ban (not just account)                                     â”‚
â”‚   â€¢ Require mutual contact for DMs                                          â”‚
â”‚                                                                             â”‚
â”‚   ILLEGAL CONTENT:                                                          â”‚
â”‚   â€¢ CSAM                                                                    â”‚
â”‚   â€¢ Terrorism-related content                                               â”‚
â”‚   â€¢ Copyright infringement                                                  â”‚
â”‚   Mitigation:                                                               â”‚
â”‚   â€¢ PhotoDNA/hashing for known illegal content                              â”‚
â”‚   â€¢ AI detection for new content                                            â”‚
â”‚   â€¢ Human review pipeline                                                   â”‚
â”‚   â€¢ Law enforcement cooperation                                             â”‚
â”‚                                                                             â”‚
â”‚   FRAUD:                                                                    â”‚
â”‚   â€¢ Impersonation                                                           â”‚
â”‚   â€¢ Phishing links                                                          â”‚
â”‚   â€¢ Scam messages                                                           â”‚
â”‚   Mitigation:                                                               â”‚
â”‚   â€¢ Verified badges for businesses                                          â”‚
â”‚   â€¢ Link safety warnings                                                    â”‚
â”‚   â€¢ Report and review pipeline                                              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Rate Abuse

```
RATE LIMITS BY CATEGORY:

Messages to contacts: 100/minute
Messages to non-contacts: 10/minute
Group creations: 10/day
Group joins: 50/day
Media uploads: 50/day, 1GB/day
Profile changes: 10/day

ABUSE PATTERNS:

Pattern: Burst of messages to 1000 users
Detection: > 100 unique recipients in 1 hour
Response: Rate limit, flag for review

Pattern: Scripted message sending
Detection: Perfectly regular intervals
Response: CAPTCHA challenge, temporary block

Pattern: Bot creating groups
Detection: Groups with same pattern (name, avatar)
Response: Account suspension
```

## Data Exposure Risks

```
RISK 1: Message Leaked to Wrong Recipient
Cause: Bug in routing logic
Impact: Critical privacy violation
Mitigation:
â€¢ Access control check at multiple layers
â€¢ Conversation membership verified on every operation
â€¢ Audit logging of all message access
â€¢ Regular security testing

RISK 2: Metadata Exposure
Cause: Logs containing message content
Impact: Privacy violation, potential legal issues
Mitigation:
â€¢ Never log message content
â€¢ Minimal metadata in logs
â€¢ Log retention limits
â€¢ Access controls on logs

RISK 3: Insider Access
Cause: Employee accesses user messages
Impact: Trust violation, legal issues
Mitigation:
â€¢ End-to-end encryption (even staff can't read)
â€¢ Access audit logs
â€¢ Principle of least privilege
â€¢ Background checks

RISK 4: API Leaking Data
Cause: API returns more than needed
Impact: Data exposure
Mitigation:
â€¢ Minimal response payloads
â€¢ Authorization on every field
â€¢ Regular API audits
```

## Privilege Boundaries

```
ACCESS CONTROL MODEL:

USER PRIVILEGES:
â€¢ Read own messages
â€¢ Send to conversations they're member of
â€¢ Manage own profile
â€¢ Block other users

GROUP ADMIN PRIVILEGES:
â€¢ Add/remove members
â€¢ Change group settings
â€¢ Delete any message in group
â€¢ Promote other admins

SYSTEM PRIVILEGES:
â€¢ Never access message content
â€¢ Can access metadata for:
  - Abuse investigation (with approval)
  - Law enforcement (with legal process)
  - Technical debugging (anonymized)

PRINCIPLE OF LEAST PRIVILEGE:
â€¢ Engineers cannot access production user data
â€¢ Debugging uses synthetic data
â€¢ Access to user data requires approval chain
â€¢ All access logged and audited
```

## Why Perfect Security Is Impossible

```
FUNDAMENTAL TENSIONS:

1. Usability vs Security
   â€¢ Perfect security: 20-character password, 2FA every time
   â€¢ Users want: Quick access, "remember this device"
   â€¢ Compromise: Risk-based authentication

2. Privacy vs Abuse Prevention
   â€¢ Perfect privacy: E2EE, no server-side inspection
   â€¢ Abuse prevention: Need to scan for illegal content
   â€¢ Compromise: Client-side scanning, or accept some abuse

3. Availability vs Security
   â€¢ Perfect security: Offline verification of everything
   â€¢ Availability: Need to work when networks are spotty
   â€¢ Compromise: Cache auth tokens, accept staleness

4. Debugging vs Privacy
   â€¢ Perfect privacy: No logs, no tracing
   â€¢ Debugging: Need to understand failures
   â€¢ Compromise: Anonymized logs, minimal retention

STAFF INSIGHT:
Security is always a trade-off. The goal is not perfect
security but appropriate security for the threat model.
Messaging has high privacy expectationsâ€”lean toward
privacy when in doubt, accept some operational complexity.
```

---

# Part 14: Evolution Over Time

## V1: Naive Design

```
INITIAL DESIGN (Startup scale: 10K users)

Architecture:
â€¢ Single server
â€¢ MySQL database
â€¢ Long polling for real-time
â€¢ Local file storage for media

Data model:
â€¢ messages table (id, sender, recipient, content, timestamp)
â€¢ users table (id, name, phone)
â€¢ No conversations abstraction

Delivery:
â€¢ Poll database every 2 seconds for new messages
â€¢ Show immediately on poll
â€¢ No delivery receipts

What works at 10K users:
â€¢ Simple to understand and debug
â€¢ Single source of truth
â€¢ All features work

What's already straining:
â€¢ Long polling creates many connections
â€¢ Database scanned on every poll
â€¢ No offline message queue
```

## What Breaks First

```
AT 100K USERS:
â€¢ Long polling: Too many connections
â€¢ Database: Query per poll kills performance
â€¢ Media: Local disk running out

FIX:
â€¢ Migrate to WebSocket (reduce connections 10x)
â€¢ Add message index by recipient
â€¢ Move media to S3

AT 1M USERS:
â€¢ Single database: Write capacity maxed
â€¢ Single server: Cannot handle connections
â€¢ Message ordering: Race conditions appearing

FIX:
â€¢ Shard database by user_id
â€¢ Multiple WebSocket servers
â€¢ Add Redis for sequence numbers

AT 10M USERS:
â€¢ Cross-shard queries: Killing performance
â€¢ Group messaging: Fan-out overwhelming
â€¢ Presence: Every status change storms

FIX:
â€¢ Denormalize inbox (fan-out on write)
â€¢ Async group delivery with queues
â€¢ Presence becomes eventually consistent
```

## V2: Improved Design

```
ARCHITECTURE (Scale: 100M users)

Components:
â€¢ WebSocket gateway cluster (100 servers)
â€¢ Message service (50 servers)
â€¢ Delivery service (50 servers)
â€¢ Presence service (Redis cluster)
â€¢ Message store (Cassandra cluster)
â€¢ Media store (S3)
â€¢ Cache layer (Redis)

Improvements over V1:
â€¢ Proper WebSocket for real-time
â€¢ Async delivery pipeline
â€¢ Conversation abstraction
â€¢ Delivery receipts
â€¢ Presence (eventually consistent)
â€¢ Multi-region awareness

Still problematic:
â€¢ Large groups are slow
â€¢ Global ordering not guaranteed
â€¢ Sync for long-offline users is slow
â€¢ Hot conversations create hotspots
```

## Long-Term Stable Architecture

```
MATURE ARCHITECTURE (Scale: 1B+ users)

Core principles:
1. Separate hot and cold paths
2. Eventual consistency by default, strong where needed
3. Cell-based architecture for isolation
4. Graceful degradation baked in

Components evolved:
â€¢ Cells: Self-contained units (10M users each)
â€¢ Cross-cell routing for conversations spanning cells
â€¢ Tiered storage (hot/warm/cold)
â€¢ Sophisticated presence (subscription-based)
â€¢ E2E encryption (client-side)
â€¢ ML-based abuse detection

Operational maturity:
â€¢ Canary deployments
â€¢ Automatic rollback on error spike
â€¢ Chaos engineering
â€¢ Runbook automation
â€¢ 24/7 on-call with clear escalation

What's stable:
â€¢ Core message flow unchanged for years
â€¢ Data model rarely changes
â€¢ Most work is optimization, not redesign
```

## How Incidents Drive Redesign

```
INCIDENT 1: New Year's Eve Outage

What happened:
â€¢ 50x traffic spike at midnight (by timezone)
â€¢ Message queue backed up
â€¢ Users saw "sending..." for minutes
â€¢ Cascaded across regions

Root cause:
â€¢ Queue capacity sized for 10x, not 50x
â€¢ No backpressure to slow senders
â€¢ Retry storms made it worse

Redesign:
â€¢ Backpressure from queue to API
â€¢ Client-side queuing with local retry
â€¢ Queue capacity 100x average
â€¢ Graceful degradation (disable typing, etc.)

INCIDENT 2: Celebrity Account Storm

What happened:
â€¢ Celebrity posted controversy
â€¢ 10M users tried to message them
â€¢ All messages routed to one shard
â€¢ Shard overloaded, affected other users on shard

Root cause:
â€¢ Sharding by recipient concentrated hot users
â€¢ No rate limiting on inbound to public figures
â€¢ No isolation between accounts

Redesign:
â€¢ Special handling for high-volume recipients
â€¢ Inbound rate limits (queue excess)
â€¢ Better shard isolation
â€¢ "Fan-out on read" for celebrity inboxes
```

---

# Part 15: Alternatives & Explicit Rejections

## Alternative 1: Pure P2P (No Central Server)

```
APPROACH:
â€¢ Messages sent directly between devices
â€¢ No central server for routing
â€¢ Device-to-device encryption built-in

WHY IT SEEMS ATTRACTIVE:
â€¢ Perfect privacy (no server access)
â€¢ Lower infrastructure cost
â€¢ No central point of failure
â€¢ Simple mental model

WHY STAFF ENGINEERS REJECT IT:
â€¢ Offline delivery: How to reach offline users?
  - Need some always-on component
â€¢ NAT traversal: Devices behind firewalls
  - Need STUN/TURN servers anyway
â€¢ Multi-device sync: How to sync phone â†” laptop?
  - Need central sync service
â€¢ Group messaging: NÂ² connections don't scale
  - Need central relay
â€¢ Abuse prevention: Can't moderate P2P
  - Platform becomes spam haven

VERDICT: Pure P2P doesn't work for mass-market messaging.
         End up building central services anyway.
         Better to design for server-assisted from start.
```

## Alternative 2: Blockchain-Based Messaging

```
APPROACH:
â€¢ Messages stored on blockchain
â€¢ Decentralized, censorship-resistant
â€¢ Cryptocurrency for spam prevention

WHY IT SEEMS ATTRACTIVE:
â€¢ "Web3" buzzword appeal
â€¢ Censorship resistance
â€¢ User ownership of data
â€¢ Built-in payment rails

WHY STAFF ENGINEERS REJECT IT:
â€¢ Latency: Block time = message delivery time
  - Even fast chains: 1-15 seconds per block
  - Unacceptable for real-time messaging
â€¢ Cost: Pay per message (gas fees)
  - $0.01/message Ã— 100B messages/day = $1B/day
â€¢ Privacy: Blockchain is public
  - Everyone can see message metadata
â€¢ Scale: Blockchains don't scale
  - Even optimistic: 10K TPS vs our 10M TPS
â€¢ Complexity: Wallets, keys, etc.
  - Mass market won't manage private keys

VERDICT: Blockchain adds cost, latency, and complexity
         with no real benefit for messaging use case.
         Censorship resistance not needed by 99% of users.
```

## Alternative 3: Actor Model (Erlang/Elixir Style)

```
APPROACH:
â€¢ Each user is an actor process
â€¢ Messages passed between actors
â€¢ Platform built on Erlang/BEAM VM

WHY IT SEEMS ATTRACTIVE:
â€¢ Natural fit for messaging domain
â€¢ Built-in fault tolerance
â€¢ Lightweight processes (millions per server)
â€¢ WhatsApp famously uses Erlang

WHY STAFF ENGINEERS ARE CAUTIOUS:
â€¢ Hiring: Erlang engineers are rare
  - Harder to scale engineering team
â€¢ Debugging: Actor systems are hard to trace
  - Distributed state is opaque
â€¢ Persistence: Still need external database
  - Actors are in-memory only
â€¢ Libraries: Ecosystem smaller than JVM/Go
  - May need to build more from scratch
â€¢ Migration: Hard to move from existing systems
  - Big-bang rewrite needed

NUANCED VERDICT:
â€¢ For greenfield: Actor model can work well
â€¢ For existing system: Migration cost too high
â€¢ Staff choice: Use actor model concepts, implement in Go/Java
  - Get benefits without platform commitment
```

---

# Part 16: Interview Calibration

## How Interviewers Probe This System

```
OPENER: "Design a messaging platform like WhatsApp"

EXPECTED FIRST QUESTIONS (L6 candidates ask these):
â€¢ "What's the expected scale? Users, messages per day?"
â€¢ "1:1 messaging, group messaging, or both?"
â€¢ "What are the latency expectations?"
â€¢ "Any regulatory/compliance requirements?"
â€¢ "Is E2E encryption in scope?"

RED FLAG FIRST QUESTIONS (L5 candidates ask these):
â€¢ "Should I use Kafka?" (jumping to implementation)
â€¢ "What database should I use?" (jumping to details)
â€¢ Starting to draw without any clarification

COMMON PROBES:
â€¢ "How do you ensure message ordering?"
â€¢ "What happens when a user is offline?"
â€¢ "How do you handle a 1000-person group?"
â€¢ "What if the database goes down?"
â€¢ "How do you prevent spam?"
â€¢ "Walk me through the delivery of a single message"
```

## Common L5 Mistakes

```
MISTAKE 1: Designing for Precision Instead of Scale
L5: "We use 2PC to ensure exactly-once delivery"
Problem: 2PC adds latency and complexity
Staff: "At-least-once with idempotent receivers is sufficient"

MISTAKE 2: Ignoring Offline Users
L5: Shows only WebSocket path
Problem: 40% of users are offline at any time
Staff: "Here's the offline queue and sync mechanism"

MISTAKE 3: Total Ordering
L5: "Single queue for all messages ensures ordering"
Problem: Single queue doesn't scale
Staff: "Per-conversation ordering with logical clocks"

MISTAKE 4: Underestimating Group Complexity
L5: "Send to each member like 1:1"
Problem: 1000-member group = 1000x fanout
Staff: "Different strategy based on group size"

MISTAKE 5: Treating All Messages Equal
L5: "All messages go through same path"
Problem: Media is 1000x larger than text
Staff: "Separate paths for text, media, signals"
```

## Staff-Level Answers

```
ON ORDERING:
"Message ordering is complex. Within a conversation, we need
per-sender ordering as a minimumâ€”messages from Alice to Bob
should appear in the order Alice sent them. For cross-sender
ordering, we use Lamport timestamps to establish causality.
The key insight is that users only care about ordering within
a single conversation viewâ€”we don't need global ordering."

ON OFFLINE HANDLING:
"Offline users are actually the common case, not the exception.
When a message arrives for an offline user, we store it in their
inbox and send a push notification. On reconnect, the client
provides its last sync cursor, and we return all messages since
then. The tricky part is handling long-offline usersâ€”we paginate
and prioritize recent conversations."

ON LARGE GROUPS:
"Large groups require different fanout strategies. For a 10-person
group, fan-out on write works fineâ€”store one message, deliver to
10 people. For a 10,000-person group, that's 10,000 deliveries
per message. Instead, we use fan-out on read: store the message
once, and each member fetches it when they open the conversation.
The threshold is around 50-100 members."

ON FAILURE:
"The non-negotiable is: never lose a message after ACK. Everything
else can degrade. During a partial outage, we'll continue to accept
and store messagesâ€”delivery might be delayed. We prioritize message
storage over delivery, and delivery over cosmetic features like
typing indicators. Users can handle a few seconds of delay; they
can't handle lost messages."
```

## Example Phrases Staff Engineers Use

```
SCOPE DEFINITION:
â€¢ "Let's define what 'delivered' means in this context..."
â€¢ "I'm going to explicitly exclude E2E encryption for now..."
â€¢ "The interesting trade-off here is between X and Y..."

TRADE-OFF ARTICULATION:
â€¢ "We could do strong consistency here, but the latency cost is..."
â€¢ "I'm choosing eventual consistency because..."
â€¢ "The risk of this approach is X, which we mitigate by Y..."

SHOWING DEPTH:
â€¢ "In my experience, what actually breaks first is..."
â€¢ "The non-obvious problem with that approach is..."
â€¢ "At first glance you might do X, but actually Y is better because..."

HANDLING UNCERTAINTY:
â€¢ "I'm not sure about the exact numbers, but order of magnitude..."
â€¢ "This would need benchmarking, but my intuition says..."
â€¢ "Let me reason through thisâ€”if we assume X, then..."

DRIVING DISCUSSION:
â€¢ "Let me walk you through the happy path first, then failures..."
â€¢ "I'll sketch the architecture, then deep-dive on [component]..."
â€¢ "Before I go further, should I elaborate on X or move to Y?"
```

---

# Part 17: Diagrams

## Diagram 1: Core Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MESSAGING PLATFORM ARCHITECTURE                          â”‚
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚                          CLIENTS                                    â”‚    â”‚
â”‚   â”‚          iOS    Android    Web    Desktop    Tablet                â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                              â”‚
â”‚                              â–¼                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚                       EDGE LAYER                                    â”‚    â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚    â”‚
â”‚   â”‚   â”‚   API    â”‚    â”‚WebSocket â”‚    â”‚   Push   â”‚                     â”‚    â”‚
â”‚   â”‚   â”‚ Gateway  â”‚    â”‚ Gateway  â”‚    â”‚ Gateway  â”‚                     â”‚    â”‚
â”‚   â”‚   â”‚  (HTTP)  â”‚    â”‚  (WS)    â”‚    â”‚(APNs/FCM)â”‚                     â”‚    â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚                      SERVICE LAYER                                  â”‚    â”‚
â”‚   â”‚                                                                     â”‚    â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚    â”‚
â”‚   â”‚  â”‚ Message  â”‚  â”‚ Delivery â”‚  â”‚ Presence â”‚  â”‚  Sync    â”‚            â”‚    â”‚
â”‚   â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚            â”‚    â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚    â”‚
â”‚   â”‚                                                                     â”‚    â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚    â”‚
â”‚   â”‚  â”‚  Group   â”‚  â”‚   User   â”‚  â”‚  Media   â”‚                          â”‚    â”‚
â”‚   â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚  â”‚ Service  â”‚                          â”‚    â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â”‚                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚                       DATA LAYER                                    â”‚    â”‚
â”‚   â”‚                                                                     â”‚    â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚    â”‚
â”‚   â”‚  â”‚ Message  â”‚  â”‚   User   â”‚  â”‚  Cache   â”‚  â”‚  Media   â”‚            â”‚    â”‚
â”‚   â”‚  â”‚  Store   â”‚  â”‚  Store   â”‚  â”‚ (Redis)  â”‚  â”‚  (S3)    â”‚            â”‚    â”‚
â”‚   â”‚  â”‚(Cassandraâ”‚  â”‚ (MySQL)  â”‚  â”‚          â”‚  â”‚          â”‚            â”‚    â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚    â”‚
â”‚   â”‚                                                                     â”‚    â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚    â”‚
â”‚   â”‚  â”‚           Message Queue (Kafka)           â”‚                      â”‚    â”‚
â”‚   â”‚  â”‚     delivery-tasks | notifications        â”‚                      â”‚    â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Diagram 2: Message Send Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MESSAGE SEND FLOW                                        â”‚
â”‚                                                                             â”‚
â”‚   ALICE (Sender)                                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                             â”‚
â”‚        â”‚                                                                    â”‚
â”‚        â”‚ 1. POST /messages                                                  â”‚
â”‚        â”‚    {conversation_id, content}                                      â”‚
â”‚        â–¼                                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                            â”‚
â”‚   â”‚    API     â”‚  2. Auth check                                             â”‚
â”‚   â”‚  Gateway   â”‚  3. Rate limit                                             â”‚
â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                            â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â”‚ 4. Send to Message Service                                        â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                            â”‚
â”‚   â”‚  Message   â”‚  5. Validate message                                       â”‚
â”‚   â”‚  Service   â”‚  6. Get sequence number (Redis INCR)                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  7. Store message (Cassandra)                              â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â”‚ 8. Return message_id to Alice                                     â”‚
â”‚         â”‚    (Alice sees âœ“ sent)                                            â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â”‚ 9. Enqueue delivery task                                          â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                            â”‚
â”‚   â”‚  Delivery  â”‚  10. Get conversation members                              â”‚
â”‚   â”‚  Service   â”‚  11. For each recipient:                                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                            â”‚
â”‚         â”‚                                                                   â”‚
â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚    â”‚                                     â”‚                                  â”‚
â”‚    â–¼                                     â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚  Presence  â”‚ 12. Is Bob online? â”‚    Push    â”‚                           â”‚
â”‚  â”‚  Service   â”‚                    â”‚  Gateway   â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚    YES â”‚                                 â”‚ NO                               â”‚
â”‚        â–¼                                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚ WebSocket  â”‚ 13. Push via WS    â”‚   APNs/    â”‚ 14. Push notification     â”‚
â”‚  â”‚  Gateway   â”‚                    â”‚    FCM     â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚        â”‚                                                                    â”‚
â”‚        â”‚ 15. Bob receives message                                           â”‚
â”‚        â–¼                                                                    â”‚
â”‚   BOB (Recipient)                                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                           â”‚
â”‚        â”‚                                                                    â”‚
â”‚        â”‚ 16. Send delivery ACK                                              â”‚
â”‚        â–¼                                                                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                            â”‚
â”‚   â”‚  Delivery  â”‚ 17. Update status â†’ DELIVERED                              â”‚
â”‚   â”‚  Service   â”‚ 18. Notify Alice (Alice sees âœ“âœ“)                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Diagram 3: Failure Propagation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FAILURE PROPAGATION                                      â”‚
â”‚                                                                             â”‚
â”‚   SCENARIO: Message Store Becomes Slow (P99 > 2 seconds)                    â”‚
â”‚                                                                             â”‚
â”‚   T+0: Message Store latency increases                                      â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  MESSAGE SERVICE                                                    â”‚   â”‚
â”‚   â”‚  â€¢ Write latency: 30ms â†’ 2000ms                                     â”‚   â”‚
â”‚   â”‚  â€¢ Thread pool saturates                                            â”‚   â”‚
â”‚   â”‚  â€¢ Requests queue up                                                â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                          â”‚
â”‚   T+30s: API Gateway affected    â”‚                                          â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  API GATEWAY                                                        â”‚   â”‚
â”‚   â”‚  â€¢ Request timeout rate: 0.1% â†’ 15%                                 â”‚   â”‚
â”‚   â”‚  â€¢ Client retries increase load                                     â”‚   â”‚
â”‚   â”‚  â€¢ Connection pool to Message Service exhausted                     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                          â”‚
â”‚   T+60s: User impact visible     â”‚                                          â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  CLIENTS                                                            â”‚   â”‚
â”‚   â”‚  â€¢ Messages stuck at "sending..."                                   â”‚   â”‚
â”‚   â”‚  â€¢ Users retry manually (more load)                                 â”‚   â”‚
â”‚   â”‚  â€¢ Angry social media posts                                         â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚   MITIGATION ACTIVATED                                                      â”‚
â”‚   â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•   â”‚
â”‚                                                                             â”‚
â”‚   T+2m: Circuit breaker triggers                                            â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  MESSAGE SERVICE                                                    â”‚   â”‚
â”‚   â”‚  â€¢ Circuit breaker OPEN                                             â”‚   â”‚
â”‚   â”‚  â€¢ Fast-fail new requests (no timeout wait)                         â”‚   â”‚
â”‚   â”‚  â€¢ Return 503 with Retry-After header                               â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                  â”‚                                          â”‚
â”‚   T+3m: Backpressure to clients  â”‚                                          â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  CLIENTS                                                            â”‚   â”‚
â”‚   â”‚  â€¢ Queue messages locally                                           â”‚   â”‚
â”‚   â”‚  â€¢ Show "connection issues" banner                                  â”‚   â”‚
â”‚   â”‚  â€¢ Retry with exponential backoff                                   â”‚   â”‚
â”‚   â”‚  â€¢ Messages saved, will send when healthy                           â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   T+10m: Message Store recovers                                             â”‚
â”‚         â”‚                                                                   â”‚
â”‚         â–¼                                                                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚  GRADUAL RECOVERY                                                   â”‚   â”‚
â”‚   â”‚  â€¢ Circuit breaker: Half-open (test requests)                       â”‚   â”‚
â”‚   â”‚  â€¢ Test requests succeed â†’ Circuit CLOSED                           â”‚   â”‚
â”‚   â”‚  â€¢ Client queues drain gradually                                    â”‚   â”‚
â”‚   â”‚  â€¢ Full recovery in ~5 minutes                                      â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚   OUTCOME:                                                                  â”‚
â”‚   â€¢ 10-minute degradation, not outage                                       â”‚
â”‚   â€¢ Zero messages lost                                                      â”‚   
â”‚   â€¢ Messages delayed 5-15 minutes                                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Diagram 4: System Evolution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SYSTEM EVOLUTION                                         â”‚
â”‚                                                                             â”‚
â”‚   V1: MONOLITH (10K users)                                                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚  Single Server                                   â”‚                      â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚                      â”‚
â”‚   â”‚  â”‚   App    â”‚  â”‚  MySQL   â”‚  â”‚  Files   â”‚        â”‚                      â”‚
â”‚   â”‚  â”‚  (PHP)   â”‚  â”‚  (1 DB)  â”‚  â”‚  (local) â”‚        â”‚                      â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                             â”‚
â”‚   â–¼ Scaling pain: DB bottleneck, file storage full                          â”‚
â”‚                                                                             â”‚
â”‚   V2: SEPARATED TIERS (1M users)                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚  API Servers â”‚  â”‚   Database   â”‚  â”‚    Media     â”‚                      â”‚
â”‚   â”‚    (x10)     â”‚  â”‚  (Primary +  â”‚  â”‚    (S3)      â”‚                      â”‚
â”‚   â”‚              â”‚  â”‚   Replica)   â”‚  â”‚              â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                             â”‚
â”‚   â–¼ Scaling pain: Long polling, real-time delays                            â”‚
â”‚                                                                             â”‚
â”‚   V3: REAL-TIME ADDED (10M users)                                           â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚   â”‚   API    â”‚  â”‚WebSocket â”‚  â”‚ Database â”‚  â”‚ Redis â”‚                       â”‚
â”‚   â”‚ Servers  â”‚  â”‚ Servers  â”‚  â”‚ (Sharded)â”‚  â”‚ Cache â”‚                       â”‚
â”‚   â”‚  (x50)   â”‚  â”‚  (x20)   â”‚  â”‚          â”‚  â”‚       â”‚                       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                             â”‚
â”‚   â–¼ Scaling pain: Group fanout, presence storms                             â”‚
â”‚                                                                             â”‚
â”‚   V4: MICROSERVICES (100M users)                                            â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚                        Services                               â”‚          â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”‚          â”‚
â”‚   â”‚  â”‚Messageâ”‚ â”‚Deliveryâ”‚ â”‚Presenceâ”‚ â”‚ Sync â”‚ â”‚ Group â”‚ â”‚ User â”‚ â”‚          â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚                     Data Stores                               â”‚          â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚          â”‚
â”‚   â”‚  â”‚Cassandraâ”‚  â”‚ MySQL  â”‚  â”‚ Redis â”‚  â”‚  S3   â”‚  â”‚   Kafka   â”‚â”‚          â”‚
â”‚   â”‚  â”‚(messagesâ”‚  â”‚(users) â”‚  â”‚(cache)â”‚  â”‚(media)â”‚  â”‚  (queue)  â”‚â”‚          â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                             â”‚
â”‚   â–¼ Scaling pain: Regional latency, compliance requirements                 â”‚
â”‚                                                                             â”‚
â”‚   V5: MULTI-REGION CELLS (1B+ users)                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚      US Cell       â”‚  â”‚      EU Cell       â”‚  â”‚    APAC Cell     â”‚      â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚
â”‚   â”‚  â”‚Full service  â”‚  â”‚  â”‚  â”‚Full service  â”‚  â”‚  â”‚ â”‚Full service  â”‚ â”‚      â”‚
â”‚   â”‚  â”‚   stack      â”‚  â”‚  â”‚  â”‚   stack      â”‚  â”‚  â”‚ â”‚   stack      â”‚ â”‚      â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚      â”‚
â”‚   â”‚  â”‚ Full data    â”‚  â”‚  â”‚  â”‚ Full data    â”‚  â”‚  â”‚ â”‚ Full data    â”‚ â”‚      â”‚
â”‚   â”‚  â”‚   replicas   â”‚â—„â”€â”¼â”€â”€â”¼â”€â”€â”¼â”€â”€â–º replicas  â”‚â—„â”€â”¼â”€â”€â”¼â”€â”¼â”€â”€â–ºreplicas   â”‚ â”‚      â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚              â–²                      â–²                      â–²                â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Cross-region replication â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 18: Brainstorming, Exercises & Redesigns

## "What If X Changes?" Questions

```
WHAT IF: Message volume 10x overnight (viral event)

Immediate impact:
â€¢ Message queues back up
â€¢ Database write capacity exceeded
â€¢ Push notification system overwhelmed

Short-term response:
â€¢ Shed non-critical features (typing, presence)
â€¢ Rate limit less-critical paths
â€¢ Scale horizontally (pre-provisioned capacity)

Long-term change:
â€¢ More aggressive auto-scaling
â€¢ Higher base capacity
â€¢ Better traffic prediction

WHAT IF: Average message size 100x (voice messages become primary)

Impact:
â€¢ Storage costs explode
â€¢ Bandwidth costs explode
â€¢ Upload/download times increase

Response:
â€¢ Aggressive compression (opus codec)
â€¢ Streaming upload/download
â€¢ Shorter retention for voice
â€¢ Separate voice message tier

WHAT IF: Regulatory requirement for message retention

Impact:
â€¢ Can't delete user messages on request
â€¢ Need audit trail for access
â€¢ Legal hold functionality needed

Response:
â€¢ Parallel compliance data store
â€¢ Access logging and approval workflow
â€¢ Separate retained vs user-visible state
â€¢ Legal review before deletion

WHAT IF: E2E encryption becomes mandatory

Impact:
â€¢ Cannot scan for abuse on server
â€¢ Cannot recover messages if user loses key
â€¢ More complex key management

Response:
â€¢ Client-side content scanning (controversial)
â€¢ Key escrow options (with user consent)
â€¢ Robust key backup mechanisms
â€¢ Accept reduced abuse detection capability
```

## Redesign Under New Constraints

```
CONSTRAINT: Zero trust network (all internal traffic encrypted)

Original: Services communicate over private network
New: Every service-to-service call authenticated + encrypted

Changes needed:
â€¢ mTLS between all services
â€¢ Service identity certificates
â€¢ Certificate rotation automation
â€¢ ~5ms latency overhead per hop

CONSTRAINT: Data must stay in user's home country

Original: Global replication, route anywhere
New: Messages stored only in user's country

Changes needed:
â€¢ Per-country data stores
â€¢ Routing based on sender country
â€¢ Cross-country conversations: ???
  - Option A: Store in sender's country (recipient must query cross-border)
  - Option B: Store copy in both countries (compliance risk?)
  - Option C: Store in neutral zone (latency, complexity)
â€¢ Significantly more operational complexity

CONSTRAINT: Maximum 10ms P99 message send latency

Original: 100ms P50, 300ms P95
New: 10ms P99

Changes needed:
â€¢ No synchronous database write (use async + local ACK)
â€¢ Edge message acceptance
â€¢ Eventual consistency everywhere
â€¢ Risk: Can lose messages if edge fails before sync

Trade-off analysis:
â€¢ 10ms P99 is unrealistic for durable messaging
â€¢ Would need to sacrifice durability guarantee
â€¢ Better: Accept 50ms P99, which is achievable with durability
```

## Failure Injection Exercises

```
EXERCISE 1: Chaos Monkey for WebSocket Gateways

Inject: Kill random WebSocket gateway every hour

Expected behavior:
â€¢ Affected users disconnect (10 seconds no messages)
â€¢ Clients auto-reconnect to different gateway
â€¢ Sync service provides missed messages
â€¢ No user-visible message loss

What to monitor:
â€¢ Reconnection success rate
â€¢ Time to reconnect
â€¢ Message loss/duplication
â€¢ User complaints

EXERCISE 2: Slow Message Store

Inject: Add 500ms latency to 10% of message store writes

Expected behavior:
â€¢ P50 latency mostly unaffected
â€¢ P90 increases noticeably
â€¢ Circuit breaker should not trigger (only 10%)
â€¢ No message loss

What to monitor:
â€¢ Latency percentiles
â€¢ Timeout rates
â€¢ User retry behavior
â€¢ Queue depths

EXERCISE 3: Push Notification Failure

Inject: Block all push notifications for 30 minutes

Expected behavior:
â€¢ Offline users don't get notified
â€¢ Online users (WebSocket) unaffected
â€¢ When push recovers, backlog clears
â€¢ Users opening app get all messages (sync)

What to monitor:
â€¢ Push delivery rate
â€¢ App open rate (users check manually?)
â€¢ Message delivery lag
â€¢ User complaints
```

## Trade-off Debates

```
DEBATE 1: Fanout on Write vs Fanout on Read

For Fanout on Write:
â€¢ Fast reads (message already in recipient's inbox)
â€¢ Simple read path
â€¢ Good for mostly 1:1 messaging

For Fanout on Read:
â€¢ Efficient writes (store once)
â€¢ Better for large groups
â€¢ Easier to handle membership changes

Staff position:
â€¢ Hybrid: FOW for small groups (<50), FOR for large groups
â€¢ 1:1 is essentially FOW (store in both inboxes)
â€¢ Threshold tuned based on actual usage patterns

DEBATE 2: Strong vs Eventual Consistency for Membership

For Strong:
â€¢ Immediate effect of blocks/bans
â€¢ No leaked messages to removed members
â€¢ Simpler mental model

For Eventual:
â€¢ Better availability
â€¢ Lower latency
â€¢ Partition tolerance

Staff position:
â€¢ Strong consistency for membership within region
â€¢ Accept eventual consistency cross-region
â€¢ Err on side of blocking (if unsure if blocked, assume blocked)

DEBATE 3: Client-Generated vs Server-Generated Message IDs

For Client-Generated:
â€¢ Idempotency built in (retry with same ID)
â€¢ Works offline (generate ID without server)
â€¢ Matches client's mental model

For Server-Generated:
â€¢ Guaranteed unique (no collision risk)
â€¢ Server controls ordering
â€¢ Simpler client

Staff position:
â€¢ Client generates ID (UUID or similar)
â€¢ Server validates uniqueness
â€¢ Benefits of idempotency outweigh collision risk
â€¢ Collision probability with UUIDv4: negligible
```

---

# Summary

Messaging platforms are deceptively complex systems. The core challenge isn't sending bytes between devicesâ€”it's maintaining ordering guarantees, handling offline users gracefully, and scaling fan-out without melting infrastructure.

**Key Staff-Level Insights:**

1. **Message delivery is sacred.** Everything elseâ€”typing indicators, presence, read receiptsâ€”can degrade. Messages must never be lost after ACK.

2. **Ordering is conversation-local.** Don't try to achieve global orderingâ€”it doesn't scale. Per-sender ordering with Lamport timestamps is sufficient.

3. **Offline is the common case.** Design the offline path first, then optimize the online path. Sync must work for users offline for days.

4. **Large groups are different.** Fan-out strategies must change based on group size. What works for 10 members fails at 10,000.

5. **Presence is eventually consistent.** Don't make presence a scaling bottleneck. Users tolerate 30-60 seconds of staleness.

6. **The client is smart.** Client-side caching, local storage, and queuing reduce server load and improve UX during failures.

**The Staff Engineer Difference:**

An L5 might design a working messaging system. An L6 designs a messaging system that gracefully degrades during failures, scales sub-linearly with traffic, handles the edge cases that break naive implementations, and evolves without requiring rewrites. The difference is in the depth of understandingâ€”not just how messages flow, but what happens when they don't.